
import os
import gc
import glob
import ast
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from tqdm.auto import tqdm
from scipy.ndimage import median_filter
from pathlib import Path
import math

# --- CONFIGURATION ---
try:
    DEVICE
except NameError:
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

TEST_CSV = '/kaggle/input/MABe-mouse-behavior-detection/test.csv'
TEST_TRACKING = '/kaggle/input/MABe-mouse-behavior-detection/test_tracking'
SUBMISSION_PATH = 'submission.csv'

def find_weights(filename='ethoswarm_v3_ep3.pth'):
    files = glob.glob(f"/kaggle/input/**/{filename}", recursive=True)
    if files: return files[0]
    return filename

WEIGHTS_PATH = find_weights('ethoswarm_v3_ep3.pth')
INFERENCE_CHUNK_SIZE = 4000
GPU_BATCH_SIZE = 8
NUM_WORKERS = 0

# --- THRESHOLDS (User Provided) ---
TH = {
    "sniff": 0.08,        "approach": 0.08,
    "rear": 0.20,         "chase": 0.10,
    "attack": 0.20,       "mount": 0.20,
    "escape": 0.10,       "avoid": 0.10,
    "climb": 0.15,        "biteobject": 0.15,
    "sniffface": 0.08,    "sniffgenital": 0.08
}
DEF_TH = 0.05

BODY_PARTS = ["ear_left", "ear_right", "nose", "neck", "body_center","lateral_left", "lateral_right", "hip_left", "hip_right","tail_base", "tail_tip"]

ACTION_LIST = sorted([
    "allogroom", "approach", "attack", "attemptmount", "avoid", "biteobject",
    "chase", "chaseattack", "climb", "defend", "dig", "disengage", "dominance",
    "dominancegroom", "dominancemount", "ejaculate", "escape", "exploreobject",
    "flinch", "follow", "freeze", "genitalgroom", "huddle", "intromit", "mount",
    "rear", "reciprocalsniff", "rest", "run", "selfgroom", "shepherd", "sniff",
    "sniffbody", "sniffface", "sniffgenital", "submit", "tussle"
])

# Separation for Masking
SELF_BEHAVIORS = sorted(["biteobject", "climb", "dig", "exploreobject", "freeze", "genitalgroom", "huddle", "rear", "rest", "run", "selfgroom"])
PAIR_BEHAVIORS = sorted(list(set(ACTION_LIST) - set(SELF_BEHAVIORS)))
SELF_IDXS = [ACTION_LIST.index(a) for a in SELF_BEHAVIORS]
PAIR_IDXS = [ACTION_LIST.index(a) for a in PAIR_BEHAVIORS]

LAB_CONFIGS = {
    "AdaptableSnail":       {"thresh": 718.59, "window": 120, "pix_cm": 14.5},
    "BoisterousParrot":     {"thresh": 50.93,  "window": 292, "pix_cm": 5.5},
    "CRIM13":               {"thresh": 207.95, "window": 117, "pix_cm": 14.5},
    "CalMS21_supplemental": {"thresh": 206.05, "window": 196, "pix_cm": 18.3},
    "CalMS21_task1":        {"thresh": 154.32, "window": 140, "pix_cm": 18.3},
    "CalMS21_task2":        {"thresh": 177.51, "window": 122, "pix_cm": 18.3},
    "CautiousGiraffe":      {"thresh": 119.97, "window": 67,  "pix_cm": 21.0},
    "DeliriousFly":         {"thresh": 97.31,  "window": 172, "pix_cm": 16.0},
    "ElegantMink":          {"thresh": 88.58,  "window": 391, "pix_cm": 18.4},
    "GroovyShrew":          {"thresh": 254.45, "window": 115, "pix_cm": 11.3},
    "InvincibleJellyfish":  {"thresh": 249.33, "window": 158, "pix_cm": 32.0},
    "JovialSwallow":        {"thresh": 99.68,  "window": 62,  "pix_cm": 15.3},
    "LyricalHare":          {"thresh": 198.80, "window": 361, "pix_cm": 10.9},
    "NiftyGoldfinch":       {"thresh": 303.02, "window": 78,  "pix_cm": 13.5},
    "PleasantMeerkat":      {"thresh": 150.58, "window": 32,  "pix_cm": 15.8},
    "ReflectiveManatee":    {"thresh": 117.76, "window": 97,  "pix_cm": 15.0},
    "SparklingTapir":       {"thresh": 281.60, "window": 252, "pix_cm": 40.0},
    "TranquilPanther":      {"thresh": 133.98, "window": 105, "pix_cm": 12.3},
    "UppityFerret":         {"thresh": 228.77, "window": 55,  "pix_cm": 12.7},
    "DEFAULT":              {"thresh": 150.00, "window": 128, "pix_cm": 15.0}
}

# FIX: Training code uses list(LAB_CONFIGS.keys()) which relies on insertion order.
TRAIN_LAB_ORDER = [
    "AdaptableSnail", "BoisterousParrot", "CRIM13", "CalMS21_supplemental",
    "CalMS21_task1", "CalMS21_task2", "CautiousGiraffe", "DeliriousFly",
    "ElegantMink", "GroovyShrew", "InvincibleJellyfish", "JovialSwallow",
    "LyricalHare", "NiftyGoldfinch", "PleasantMeerkat", "ReflectiveManatee",
    "SparklingTapir", "TranquilPanther", "UppityFerret", "DEFAULT"
]
LAB_NAME_TO_IDX = {name: i for i, name in enumerate(TRAIN_LAB_ORDER)}

# ==============================================================================
# MODEL ARCHITECTURE (COPIED FROM TRAINING CODE)
# ==============================================================================
class CanonicalGraphAdapter(nn.Module):
    def __init__(self, input_nodes=11, canonical_nodes=11, feat_dim=16, num_labs=20):
        super().__init__()
        self.projection = nn.Parameter(torch.eye(input_nodes).unsqueeze(0).repeat(num_labs, 1, 1))
        self.projection.data += torch.randn_like(self.projection) * 0.01
        self.bias = nn.Parameter(torch.zeros(num_labs, 1, canonical_nodes, feat_dim))
        self.refine = nn.Sequential(
            nn.Linear(feat_dim, feat_dim * 2),
            nn.LayerNorm(feat_dim * 2),
            nn.GELU(),
            nn.Linear(feat_dim * 2, feat_dim)
        )

    def forward(self, x, lab_idx):
        b, t, n, f = x.shape
        W = self.projection[lab_idx]
        B = self.bias[lab_idx]
        x_flat = x.view(-1, n, f)
        W_flat = W.unsqueeze(1).repeat(1, t, 1, 1).view(-1, n, n)
        x_t = x_flat.transpose(1, 2)
        out = torch.bmm(x_t, W_flat)
        out = out.transpose(1, 2).view(b, t, n, f)
        out = out + B
        out = self.refine(out)
        return out

class SocialInteractionBlock(nn.Module):
    def __init__(self, node_dim=16, hidden_dim=64):
        super().__init__()
        self.relational_mlp = nn.Sequential(
            nn.Linear(5, 32),
            nn.GELU(),
            nn.Linear(32, 16)
        )
        self.fusion = nn.Linear(node_dim * 2 + 16, hidden_dim)

    def forward(self, agent_canon, target_canon):
        interaction_raw = agent_canon[..., 4:7].mean(dim=2)
        vel_self = agent_canon[..., 2:4].mean(dim=2)
        vel_targ = target_canon[..., 2:4].mean(dim=2)
        speed_diff = torch.norm(vel_self - vel_targ, dim=-1, keepdim=True)
        dot_prod = (vel_self * vel_targ).sum(dim=-1, keepdim=True)
        rel_feats = torch.cat([interaction_raw, speed_diff, dot_prod], dim=-1)
        rel_embed = self.relational_mlp(rel_feats)
        return agent_canon, target_canon, rel_embed

class MorphologicalInteractionCore(nn.Module):
    def __init__(self, num_labs=20):
        super().__init__()
        self.adapter = CanonicalGraphAdapter(input_nodes=11, canonical_nodes=11, num_labs=num_labs)
        self.interaction = SocialInteractionBlock()
        self.frame_fusion = nn.Linear(368, 128)

    def forward(self, agent_x, target_x, lab_idx):
        a_c = self.adapter(agent_x, lab_idx)
        t_c = self.adapter(target_x, lab_idx)
        _, _, rel_embed = self.interaction(a_c, t_c)
        b, t, n, f = a_c.shape
        a_flat = a_c.view(b, t, -1)
        t_flat = t_c.view(b, t, -1)
        combined = torch.cat([a_flat, t_flat, rel_embed], dim=-1)
        out = self.frame_fusion(combined)
        return out, a_c, t_c

class SplitStreamInteractionBlock(nn.Module):
    def __init__(self, node_dim=16, hidden_dim=128):
        super(SplitStreamInteractionBlock, self).__init__()
        self.self_input_size = 11 * 4
        self.self_projector = nn.Sequential(
            nn.Linear(self.self_input_size, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Linear(hidden_dim, hidden_dim)
        )
        self.relational_mlp = nn.Sequential(
            nn.Linear(3, 32),
            nn.GELU(),
            nn.Linear(32, 32)
        )
        full_node_dim = 11 * node_dim
        pair_input_dim = (full_node_dim * 2) + 32 + 2
        self.pair_projector = nn.Sequential(
            nn.Linear(pair_input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Linear(hidden_dim, hidden_dim)
        )
        self.role_embedding = nn.Parameter(torch.tensor([[1.0, 0.0], [0.0, 1.0]]))

    def forward(self, agent_c, target_c):
        batch, time, nodes, feat = agent_c.shape
        agent_proprioception = agent_c[..., 0:4]
        agent_flat_self = agent_proprioception.contiguous().view(batch, time, -1)
        self_feat = self.self_projector(agent_flat_self)
        agent_flat_full = agent_c.view(batch, time, -1)
        target_flat_full = target_c.view(batch, time, -1)
        rel_feats = agent_c[..., 4:7].mean(dim=2)
        rel_embed = self.relational_mlp(rel_feats)
        role_token = self.role_embedding[0].view(1, 1, 2).expand(batch, time, 2)
        pair_input = torch.cat([
            agent_flat_full,
            target_flat_full,
            rel_embed,
            role_token
        ], dim=-1)
        pair_feat = self.pair_projector(pair_input)
        return self_feat, pair_feat

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=10000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

    def forward(self, x):
        L = x.size(1)
        if L > self.pe.size(0):
            return x + self.pe[:self.pe.size(0), :].repeat(math.ceil(L/self.pe.size(0)), 1)[:L, :]
        return x + self.pe[:L, :]

class LocalGlobalChronosEncoder(nn.Module):
    def __init__(self, input_dim=128, hidden_dim=128):
        super(LocalGlobalChronosEncoder, self).__init__()
        self.global_proj = nn.Linear(input_dim, hidden_dim)
        self.pos_encoder = PositionalEncoding(hidden_dim, max_len=5000)
        global_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=4,
            dim_feedforward=512,
            batch_first=True,
            dropout=0.1,
            activation="gelu"
        )
        self.global_transformer = nn.TransformerEncoder(global_layer, num_layers=2)
        self.self_tcn = nn.Sequential(
            nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1),
            nn.BatchNorm1d(hidden_dim),
            nn.GELU(),
            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=2, dilation=2),
            nn.BatchNorm1d(hidden_dim),
            nn.GELU(),
            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=4, dilation=4),
            nn.BatchNorm1d(hidden_dim),
            nn.GELU(),
            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=8, dilation=8),
            nn.BatchNorm1d(hidden_dim),
            nn.GELU(),
            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=16, dilation=16),
            nn.BatchNorm1d(hidden_dim),
            nn.GELU()
        )
        self.self_attn = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=4, batch_first=True)
        self.self_norm = nn.LayerNorm(hidden_dim)
        self.pair_tcn = nn.Sequential(
            nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1),
            nn.BatchNorm1d(hidden_dim),
            nn.GELU(),
            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=2, dilation=2),
            nn.BatchNorm1d(hidden_dim),
            nn.GELU(),
            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=4, dilation=4),
            nn.BatchNorm1d(hidden_dim),
            nn.GELU(),
            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=8, dilation=8),
            nn.BatchNorm1d(hidden_dim),
            nn.GELU(),
            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=16, dilation=16),
            nn.BatchNorm1d(hidden_dim),
            nn.GELU()
        )
        self.pair_attn = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=4, batch_first=True)
        self.pair_norm = nn.LayerNorm(hidden_dim)

    def forward(self, global_feat, local_self, local_pair):
        g_emb = self.global_proj(global_feat)
        g_emb = self.pos_encoder(g_emb)
        global_memory = self.global_transformer(g_emb)
        s_in = local_self.permute(0, 2, 1)
        s_tcn = self.self_tcn(s_in).permute(0, 2, 1)
        s_ctx, _ = self.self_attn(query=s_tcn, key=global_memory, value=global_memory)
        self_out = self.self_norm(s_tcn + s_ctx)
        p_in = local_pair.permute(0, 2, 1)
        p_tcn = self.pair_tcn(p_in).permute(0, 2, 1)
        p_ctx, _ = self.pair_attn(query=p_tcn, key=global_memory, value=global_memory)
        pair_out = self.pair_norm(p_tcn + p_ctx)
        return self_out, pair_out

class MultiTaskLogicHead(nn.Module):
    def __init__(self, input_dim=128, num_labs=20):
        super(MultiTaskLogicHead, self).__init__()
        self.lab_embedding = nn.Embedding(num_labs, 32)
        fusion_dim = input_dim + 32
        expanded_dim = 256
        self.self_classifier = nn.Sequential(
            nn.Linear(fusion_dim, expanded_dim),
            nn.LayerNorm(expanded_dim),
            nn.GELU(),
            nn.Linear(expanded_dim, 11)
        )
        self.pair_classifier = nn.Sequential(
            nn.Linear(fusion_dim, expanded_dim),
            nn.LayerNorm(expanded_dim),
            nn.GELU(),
            nn.Linear(expanded_dim, 26)
        )
        self.center_regressor = nn.Sequential(
            nn.Linear(fusion_dim, 64),
            nn.GELU(),
            nn.Linear(64, 1)
        )
        self.gate_control = nn.Linear(1, 1)

    def forward(self, self_feat, pair_feat, lab_idx, agent_c, target_c):
        batch, time, _ = self_feat.shape
        lab_context = self.lab_embedding(lab_idx).unsqueeze(1).expand(-1, time, -1)
        self_input = torch.cat([self_feat, lab_context], dim=-1)
        pair_input = torch.cat([pair_feat, lab_context], dim=-1)
        self_logits = self.self_classifier(self_input)
        pair_logits = self.pair_classifier(pair_input)
        center_score = torch.sigmoid(self.center_regressor(pair_input))
        a_pos = agent_c[:, :, 0, :2]
        t_pos = target_c[:, :, 0, :2]
        dist = torch.norm(a_pos - t_pos, dim=-1, keepdim=True)
        gate = torch.sigmoid(self.gate_control(dist))
        self_probs = torch.sigmoid(self_logits)
        pair_probs = torch.sigmoid(pair_logits) * gate
        return self_probs, pair_probs, center_score

class EthoSwarmNet(nn.Module):
    def __init__(self, num_classes=37, input_dim=128):
        super(EthoSwarmNet, self).__init__()
        self.morph_core = MorphologicalInteractionCore(num_labs=20)
        self.split_interaction = SplitStreamInteractionBlock(hidden_dim=128)
        self.chronos = LocalGlobalChronosEncoder(input_dim=128, hidden_dim=128)
        self.logic_head = MultiTaskLogicHead(input_dim=128, num_labs=20)
        self.register_buffer('self_indices', self._get_indices(SELF_BEHAVIORS))
        self.register_buffer('pair_indices', self._get_indices(PAIR_BEHAVIORS))

    def _get_indices(self, subset_list):
        indices = []
        for beh in subset_list:
            try:
                indices.append(ACTION_LIST.index(beh))
            except ValueError:
                pass
        return torch.tensor(indices, dtype=torch.long)

    def forward(self, global_agent, global_target, local_agent, local_target, lab_idx):
        g_out, _, _ = self.morph_core(global_agent, global_target, lab_idx)
        _, l_ac, l_tc = self.morph_core(local_agent, local_target, lab_idx)
        l_self, l_pair = self.split_interaction(l_ac, l_tc)
        t_self, t_pair = self.chronos(g_out, l_self, l_pair)
        p_self, p_pair, center_score = self.logic_head(t_self, t_pair, lab_idx, l_ac, l_tc)
        batch, time, _ = p_self.shape
        final_output = torch.zeros(batch, time, 37, device=p_self.device, dtype=p_self.dtype)
        final_output.index_copy_(2, self.self_indices, p_self)
        final_output.index_copy_(2, self.pair_indices, p_pair)
        return final_output

# ==============================================================================
# DATASET & UTILS
# ==============================================================================
def parse_test_tasks(test_csv_path):
    if not os.path.exists(test_csv_path): return []
    df = pd.read_csv(test_csv_path)
    tasks = []

    print(f"Scanning {len(df)} videos for tasks...")
    for i, row in df.iterrows():
        vid = str(row['video_id'])
        lab = row['lab_id']
        raw = row.get('behaviors_labeled', None)
        try:
            if pd.isna(raw):
                tasks.append({'vid': vid, 'lab': lab, 'ag': 'mouse1', 'tg': 'mouse2'})
                tasks.append({'vid': vid, 'lab': lab, 'ag': 'mouse2', 'tg': 'mouse1'})
                continue

            labels = ast.literal_eval(raw)
            seen = set()
            for item in labels:
                if isinstance(item, str): parts = item.split(',')
                else: parts = item

                if len(parts) >= 2:
                    ag = parts[0].strip()
                    tg = parts[1].strip()
                    if tg.lower() == 'self' or tg == '': tg = 'self'

                    if (ag, tg) not in seen:
                        tasks.append({'vid': vid, 'lab': lab, 'ag': ag, 'tg': tg})
                        seen.add((ag, tg))
        except:
            tasks.append({'vid': vid, 'lab': lab, 'ag': 'mouse1', 'tg': 'mouse2'})

    print(f"Total Unique Tasks: {len(tasks)}")
    return tasks

class TaskInferenceDataset(Dataset):
    def __init__(self, tasks, tracking_dir):
        self.samples = tasks
        self.tracking_dir = Path(tracking_dir)
        self.bp_idx = {b:i for i,b in enumerate(BODY_PARTS)}
        self.ALIASES = {'hip_left': ['lateral_left', 'tail_base'], 'hip_right': ['lateral_right', 'tail_base'], 'neck': ['headpiece_bottombackright', 'nose'], 'body_center': ['tail_base']}

    def __len__(self): return len(self.samples)

    def _fix_teleport(self, pos):
        T, N, _ = pos.shape
        missing = (np.abs(pos).sum(axis=2) < 1e-6)
        cleaned = pos.copy()
        for n in range(N):
            m = missing[:, n]
            if np.any(m) and not np.all(m):
                valid_t = np.where(~m)[0]
                missing_t = np.where(m)[0]
                cleaned[missing_t, n, 0] = np.interp(missing_t, valid_t, pos[valid_t, n, 0])
                cleaned[missing_t, n, 1] = np.interp(missing_t, valid_t, pos[valid_t, n, 1])
        return cleaned

    def _geo_features(self, m1, m2, lab):
        # NORMALIZED SCALE
        conf = LAB_CONFIGS.get(lab, LAB_CONFIGS['DEFAULT'])
        px = float(conf['pix_cm']) + 1e-6
        m1 = m1.astype(np.float32) / px
        m2 = m2.astype(np.float32) / px

        # Align (Matches Training: Centering only)
        origin = m1[:, 9:10] # Tail base
        centered = m1 - origin
        other_centered = m2 - origin # Not used in features, but good for visualization

        # Velocity
        vel = np.diff(centered, axis=0, prepend=centered[0:1])
        speed = np.sqrt((vel**2).sum(axis=-1))

        # Relation
        dist = np.sqrt(((m1 - m2)**2).sum(axis=-1))

        # Pack to 16 matching Training
        # [Pos X, Pos Y, Vel X, Vel Y, Speed, Rel_Dist] + Pads
        feats = np.stack([
            centered[...,0], centered[...,1],
            vel[...,0], vel[...,1],
            speed, dist
        ], axis=-1)

        L, N, _ = m1.shape
        padded = np.zeros((L, N, 16), dtype=np.float32)
        padded[..., :6] = feats

        padded = np.nan_to_num(padded, nan=0.0)
        padded = np.clip(padded, -30.0, 30.0)

        return padded

    def __getitem__(self, idx):
        job = self.samples[idx]
        job_out = job.copy()

        try:
            p = self.tracking_dir / job['lab'] / f"{job['vid']}.parquet"
            if not p.exists(): return torch.zeros(1), 0, job_out

            df = pd.read_parquet(p)
            max_f = int(df.video_frame.max() + 1) if 'video_frame' in df else len(df)
            limit = min(max_f, 3000000)

            uids = df.mouse_id.unique()
            def resolve(req):
                if req == 'self': return None
                for u in uids:
                    if str(u) == str(req): return u
                    if f"mouse{u}" == str(req): return u
                return uids[0]

            real_ag = resolve(job['ag'])
            real_tg = resolve(job['tg'])

            if real_tg is None:
                real_tg = real_ag
                job_out['is_self'] = True
            else:
                job_out['is_self'] = False

            job_out['sub_ag'] = f"mouse{real_ag}" if str(real_ag).isdigit() else str(real_ag)
            job_out['sub_tg'] = 'self' if job_out['is_self'] else (f"mouse{real_tg}" if str(real_tg).isdigit() else str(real_tg))

            raw_ag = np.zeros((limit, 11, 2), dtype=np.float32)
            raw_tg = np.zeros_like(raw_ag)
            avail = set(df['bodypart'].unique())

            def extract(mid, dest):
                m_rows = df[df.mouse_id == mid]
                groups = dict(tuple(m_rows.groupby('bodypart')))
                for bp, idx in self.bp_idx.items():
                    src = bp
                    if src not in avail and bp in self.ALIASES:
                        for a in self.ALIASES[bp]:
                            if a in avail: src = a; break
                    if src in groups:
                        g = groups[src]
                        f = g.video_frame.values.astype(int) if 'video_frame' in g else np.arange(len(g))
                        v = f < limit
                        dest[f[v], idx] = g[['x','y']].values[v]

            extract(real_ag, raw_ag)
            extract(real_tg, raw_tg)

            raw_ag = self._fix_teleport(raw_ag)
            raw_tg = self._fix_teleport(raw_tg)

            # Fill missing tail bases if possible (training didn't explicit do this but it helps robustness)
            for m in [raw_ag, raw_tg]:
                tail_base = m[:, 9]
                for i in [3, 4, 7, 8]:
                    if m[:, i].sum() == 0: m[:, i] = tail_base

            feats = self._geo_features(raw_ag, raw_tg, job['lab'])
            lid = LAB_NAME_TO_IDX.get(job['lab'], 0)

            return torch.tensor(feats), lid, job_out

        except: return torch.zeros(1), 0, job_out

def collate_fn(b): return b[0]

# ==============================================================================
# MAIN
# ==============================================================================
if __name__ == "__main__":
    print(f"--- STARTING MODULE 8: FINAL INFERENCE (ROBUST) ---")

    if not os.path.exists(TEST_CSV):
        pd.DataFrame(columns=['row_id','video_id','action']).to_csv(SUBMISSION_PATH, index=False); exit()

    tasks = parse_test_tasks(TEST_CSV)
    ds = TaskInferenceDataset(tasks, TEST_TRACKING)
    loader = DataLoader(ds, batch_size=1, collate_fn=collate_fn, num_workers=NUM_WORKERS)

    try: model = EthoSwarmNet(37, 128).to(DEVICE)
    except: print("ERROR: EthoSwarmNet not found."); exit()

    if os.path.exists(WEIGHTS_PATH):
        try:
            model.load_state_dict(torch.load(WEIGHTS_PATH, map_location=DEVICE), strict=False)
            print(f"Weights loaded from {WEIGHTS_PATH}")
        except: pass
    model.eval()

    first_pass = True
    with open(SUBMISSION_PATH, 'w') as f:
        f.write("row_id,video_id,agent_id,target_id,action,start_frame,stop_frame\n")
        row_id = 0

        with torch.no_grad():
            for feat, lid, job in tqdm(loader, desc="Inferring"):
                if feat.dim() < 3 or feat.shape[0] < 5: continue

                T = feat.shape[0]
                # Pad for chunking
                rem = T % INFERENCE_CHUNK_SIZE
                if rem > 0:
                    pad = INFERENCE_CHUNK_SIZE - rem
                    feat = torch.cat([feat, feat[-1:].repeat(pad,1,1)], 0)

                c_f = feat.view(-1, INFERENCE_CHUNK_SIZE, 11, 16)
                preds_list = []
                for i in range(len(c_f)):
                    ba = c_f[i:i+1].to(DEVICE)
                    l_idx = torch.tensor([lid], device=DEVICE)
                    out = model(ba, ba, ba, ba, l_idx)
                    preds_list.append(out.cpu())

                # Raw Probs
                probs = torch.cat(preds_list, 1).flatten(0, 1)[:T].numpy()

                # 1. Strict Masking
                if job['is_self']:
                    probs[:, PAIR_IDXS] = 0.0
                else:
                    probs[:, SELF_IDXS] = 0.0

                # 2. TOP-1 GATING
                # Keep only the single highest probability per frame
                topk_vals = np.sort(probs, axis=1)[:, -1:]
                mask_k = (probs >= topk_vals).astype(np.float32)
                probs *= mask_k

                # 3. Debug
                if first_pass:
                    print(f"\n--- DEBUG SNAPSHOT ({job['vid']}) ---")
                    print(f"Task: {job['sub_ag']} -> {job['sub_tg']}")
                    mid = T // 2
                    top = np.argmax(probs[mid])
                    print(f"Frame {mid} Winner: {ACTION_LIST[top]} ({probs[mid, top]:.3f})")
                    first_pass = False

                aid, tid = job['sub_ag'], job['sub_tg']

                for cls_idx, act in enumerate(ACTION_LIST):
                    trace = probs[:, cls_idx]
                    if trace.max() < 0.01: continue

                    thresh = TH.get(act, DEF_TH)

                    smoothed = median_filter(trace, size=7)
                    binary = (smoothed > thresh).astype(np.int8)

                    if binary.sum() == 0: continue
                    diffs = np.diff(np.concatenate(([0], binary, [0])))
                    starts = np.where(diffs == 1)[0]
                    ends = np.where(diffs == -1)[0]

                    for s, e in zip(starts, ends):
                        if (e - s) < 2: continue
                        f.write(f"{row_id},{job['vid']},{aid},{tid},{act},{s},{e}\n")
                        row_id += 1

    print(f"Success! Generated {row_id} rows.")
    print(f"Submission saved to {SUBMISSION_PATH}")
