{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85e2b166",
   "metadata": {
    "id": "CJTxnCW5Oh1W",
    "papermill": {
     "duration": 0.008759,
     "end_time": "2025-12-09T11:34:17.036741",
     "exception": false,
     "start_time": "2025-12-09T11:34:17.027982",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Module 1: The Bio-Physics Data Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f0abbc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T11:34:17.055568Z",
     "iopub.status.busy": "2025-12-09T11:34:17.055191Z",
     "iopub.status.idle": "2025-12-09T11:34:22.057848Z",
     "shell.execute_reply": "2025-12-09T11:34:22.057231Z"
    },
    "id": "hnCptjwOOh1X",
    "papermill": {
     "duration": 5.014896,
     "end_time": "2025-12-09T11:34:22.059274",
     "exception": false,
     "start_time": "2025-12-09T11:34:17.044378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import ast\n",
    "import random\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "LAB_CONFIGS = {\n",
    "    \"AdaptableSnail\":       {\"thresh\": 718.59, \"window\": 120, \"pix_cm\": 14.5},\n",
    "    \"BoisterousParrot\":     {\"thresh\": 50.93,  \"window\": 292, \"pix_cm\": 5.5},\n",
    "    \"CRIM13\":               {\"thresh\": 207.95, \"window\": 117, \"pix_cm\": 14.5},\n",
    "    \"CalMS21_supplemental\": {\"thresh\": 206.05, \"window\": 196, \"pix_cm\": 18.3},\n",
    "    \"CalMS21_task1\":        {\"thresh\": 154.32, \"window\": 140, \"pix_cm\": 18.3},\n",
    "    \"CalMS21_task2\":        {\"thresh\": 177.51, \"window\": 122, \"pix_cm\": 18.3},\n",
    "    \"CautiousGiraffe\":      {\"thresh\": 119.97, \"window\": 67,  \"pix_cm\": 21.0},\n",
    "    \"DeliriousFly\":         {\"thresh\": 97.31,  \"window\": 172, \"pix_cm\": 16.0},\n",
    "    \"ElegantMink\":          {\"thresh\": 88.58,  \"window\": 391, \"pix_cm\": 18.4},\n",
    "    \"GroovyShrew\":          {\"thresh\": 254.45, \"window\": 115, \"pix_cm\": 11.3},\n",
    "    \"InvincibleJellyfish\":  {\"thresh\": 249.33, \"window\": 158, \"pix_cm\": 32.0},\n",
    "    \"JovialSwallow\":        {\"thresh\": 99.68,  \"window\": 62,  \"pix_cm\": 15.3},\n",
    "    \"LyricalHare\":          {\"thresh\": 198.80, \"window\": 361, \"pix_cm\": 10.9},\n",
    "    \"NiftyGoldfinch\":       {\"thresh\": 303.02, \"window\": 78,  \"pix_cm\": 13.5},\n",
    "    \"PleasantMeerkat\":      {\"thresh\": 150.58, \"window\": 32,  \"pix_cm\": 15.8},\n",
    "    \"ReflectiveManatee\":    {\"thresh\": 117.76, \"window\": 97,  \"pix_cm\": 15.0},\n",
    "    \"SparklingTapir\":       {\"thresh\": 281.60, \"window\": 252, \"pix_cm\": 40.0},\n",
    "    \"TranquilPanther\":      {\"thresh\": 133.98, \"window\": 105, \"pix_cm\": 12.3},\n",
    "    \"UppityFerret\":         {\"thresh\": 228.77, \"window\": 55,  \"pix_cm\": 12.7},\n",
    "    \"DEFAULT\":              {\"thresh\": 150.00, \"window\": 128, \"pix_cm\": 15.0}\n",
    "}\n",
    "\n",
    "ACTION_LIST = sorted([\n",
    "    \"allogroom\", \"approach\", \"attack\", \"attemptmount\", \"avoid\", \"biteobject\",\n",
    "    \"chase\", \"chaseattack\", \"climb\", \"defend\", \"dig\", \"disengage\", \"dominance\",\n",
    "    \"dominancegroom\", \"dominancemount\", \"ejaculate\", \"escape\", \"exploreobject\",\n",
    "    \"flinch\", \"follow\", \"freeze\", \"genitalgroom\", \"huddle\", \"intromit\", \"mount\",\n",
    "    \"rear\", \"reciprocalsniff\", \"rest\", \"run\", \"selfgroom\", \"shepherd\", \"sniff\",\n",
    "    \"sniffbody\", \"sniffface\", \"sniffgenital\", \"submit\", \"tussle\"\n",
    "])\n",
    "ACTION_TO_IDX = {a: i for i, a in enumerate(ACTION_LIST)}\n",
    "NUM_CLASSES = len(ACTION_LIST)\n",
    "BODY_PARTS = [\n",
    "    \"ear_left\", \"ear_right\", \"nose\", \"neck\", \"body_center\",\n",
    "    \"lateral_left\", \"lateral_right\", \"hip_left\", \"hip_right\",\n",
    "    \"tail_base\", \"tail_tip\"\n",
    "]\n",
    "PART_TO_IDX = {p: i for i, p in enumerate(BODY_PARTS)}\n",
    "\n",
    "class BioPhysicsDataset(Dataset):\n",
    "    def __init__(self, data_root, mode='train', video_ids=None):\n",
    "        self.root = Path(data_root)\n",
    "        self.mode = mode\n",
    "        # Directory logic\n",
    "        self.tracking_dir = self.root / f\"{mode}_tracking\"\n",
    "        self.annot_dir = self.root / f\"{mode}_annotation\"\n",
    "        \n",
    "        # Load Metadata\n",
    "        self.metadata = pd.read_csv(self.root / f\"{mode}.csv\")\n",
    "        \n",
    "        # Filter Video IDs (e.g., for train/val split)\n",
    "        if video_ids is not None:\n",
    "            self.metadata = self.metadata[self.metadata['video_id'].astype(str).isin(video_ids)]\n",
    "        \n",
    "        # Build samples from metadata DIRECTLY (Skip strict file check to avoid crash)\n",
    "        self.samples = []\n",
    "        for _, row in self.metadata.iterrows():\n",
    "            self.samples.append({\n",
    "                'video_id': str(row['video_id']),\n",
    "                'lab_id': row['lab_id']\n",
    "            })\n",
    "            \n",
    "        # Hardcoded Window\n",
    "        self.local_window = 256\n",
    "        self.max_global_tokens = 2048\n",
    "\n",
    "        # Pre-scan for sampling\n",
    "        self.action_windows = []\n",
    "        if self.mode == 'train':\n",
    "            self._scan_actions_safe()\n",
    "\n",
    "    def _scan_actions_safe(self):\n",
    "        # We try to find files. If not found, we skip optimization, but DO NOT CRASH.\n",
    "        count = 0\n",
    "        print(\"Scanning subset of annotations for sampling...\")\n",
    "        for i, s in enumerate(self.samples):\n",
    "            if i > 500: break # Quick partial scan\n",
    "            p = self.annot_dir / s['lab_id'] / f\"{s['video_id']}.parquet\"\n",
    "            if p.exists():\n",
    "                try:\n",
    "                    df = pd.read_parquet(p)\n",
    "                    # Find centers\n",
    "                    df = df[df['action'].isin(ACTION_TO_IDX)]\n",
    "                    if not df.empty:\n",
    "                        for c in ((df['start_frame'] + df['stop_frame']) // 2).values:\n",
    "                            self.action_windows.append((i, int(c)))\n",
    "                            count += 1\n",
    "                except: pass\n",
    "        if count == 0:\n",
    "            print(\"Warning: No actions scanned. Falling back to random sampling.\")\n",
    "\n",
    "    def _fix_teleport(self, pos):\n",
    "        # pos: [T, 11, 2]\n",
    "        T, N, _ = pos.shape\n",
    "        # Identify holes\n",
    "        missing = (np.abs(pos).sum(axis=2) < 1e-6)\n",
    "        cleaned = pos.copy()\n",
    "        for n in range(N):\n",
    "            m = missing[:, n]\n",
    "            if np.any(m) and not np.all(m):\n",
    "                valid_t = np.where(~m)[0]\n",
    "                missing_t = np.where(m)[0]\n",
    "                cleaned[missing_t, n, 0] = np.interp(missing_t, valid_t, pos[valid_t, n, 0])\n",
    "                cleaned[missing_t, n, 1] = np.interp(missing_t, valid_t, pos[valid_t, n, 1])\n",
    "        return cleaned\n",
    "\n",
    "    def _geo_feats(self, pos, other, pix_cm):\n",
    "        # Simple geometric extractor\n",
    "        # Normalize\n",
    "        pos = pos / pix_cm\n",
    "        other = other / pix_cm\n",
    "        \n",
    "        # Align\n",
    "        origin = pos[:, 9:10, :] # Tail base\n",
    "        centered = pos - origin\n",
    "        other_centered = other - origin\n",
    "        \n",
    "        # Velocity\n",
    "        vel = np.diff(centered, axis=0, prepend=centered[0:1])\n",
    "        speed = np.sqrt((vel**2).sum(axis=-1))\n",
    "        \n",
    "        # Relation\n",
    "        dist = np.sqrt(((pos - other)**2).sum(axis=-1))\n",
    "        \n",
    "        # Pack to 16\n",
    "        # [Pos X, Pos Y, Vel X, Vel Y, Speed, Rel_Dist] + Pads\n",
    "        feat = np.stack([\n",
    "            centered[...,0], centered[...,1],\n",
    "            vel[...,0], vel[...,1],\n",
    "            speed, dist,\n",
    "            np.zeros_like(speed), np.zeros_like(speed), # 7-8\n",
    "            np.zeros_like(speed), np.zeros_like(speed),\n",
    "            np.zeros_like(speed), np.zeros_like(speed),\n",
    "            np.zeros_like(speed), np.zeros_like(speed),\n",
    "            np.zeros_like(speed), np.zeros_like(speed),\n",
    "        ], axis=-1)\n",
    "        \n",
    "        return feat.astype(np.float32)\n",
    "\n",
    "    def _load(self, idx, center=None):\n",
    "        sample = self.samples[idx]\n",
    "        lab = sample['lab_id']\n",
    "        conf = LAB_CONFIGS.get(lab, LAB_CONFIGS['DEFAULT'])\n",
    "        \n",
    "        # Try Loading Track\n",
    "        raw_m1, raw_m2 = np.zeros((1,11,2)), np.zeros((1,11,2))\n",
    "        \n",
    "        fpath = self.tracking_dir / lab / f\"{sample['video_id']}.parquet\"\n",
    "        \n",
    "        # Load Success?\n",
    "        success = False\n",
    "        if fpath.exists():\n",
    "            try:\n",
    "                df = pd.read_parquet(fpath)\n",
    "                mids = df['mouse_id'].unique()\n",
    "                L = len(df)\n",
    "                \n",
    "                # Expand buffer\n",
    "                raw_m1 = np.zeros((L, 11, 2), dtype=np.float32)\n",
    "                raw_m2 = np.zeros((L, 11, 2), dtype=np.float32)\n",
    "                \n",
    "                m1_id = mids[0]\n",
    "                m2_id = mids[1] if len(mids) > 1 else m1_id\n",
    "                \n",
    "                # Check Bodypart column\n",
    "                if 'bodypart' in df.columns:\n",
    "                    for i, bp in enumerate(BODY_PARTS):\n",
    "                        d1 = df[(df['mouse_id']==m1_id) & (df['bodypart']==bp)][['x','y']].values\n",
    "                        if len(d1)>0: raw_m1[:len(d1), i] = d1\n",
    "                        \n",
    "                        d2 = df[(df['mouse_id']==m2_id) & (df['bodypart']==bp)][['x','y']].values\n",
    "                        if len(d2)>0: raw_m2[:len(d2), i] = d2\n",
    "                else:\n",
    "                    # Wide format check\n",
    "                    for col in df.columns:\n",
    "                        if \"mouse1\" in col:\n",
    "                            # simplified parsing\n",
    "                            pass \n",
    "                success = True\n",
    "            except: pass\n",
    "        \n",
    "        if not success:\n",
    "            # DUMMY DATA TO PREVENT CRASH\n",
    "            # Returns a single frame of zeros\n",
    "            L = self.local_window\n",
    "            raw_m1 = np.zeros((L, 11, 2), dtype=np.float32)\n",
    "            raw_m2 = np.zeros((L, 11, 2), dtype=np.float32)\n",
    "\n",
    "        # 1. Teleport Fix\n",
    "        raw_m1 = self._fix_teleport(raw_m1)\n",
    "        raw_m2 = self._fix_teleport(raw_m2)\n",
    "        \n",
    "        # 2. Window\n",
    "        seq_len = len(raw_m1)\n",
    "        if center is None: center = random.randint(0, seq_len)\n",
    "        s = max(0, min(center - self.local_window//2, seq_len - self.local_window))\n",
    "        e = min(s + self.local_window, seq_len)\n",
    "        \n",
    "        idx_slice = np.arange(s, e)\n",
    "        \n",
    "        # 3. Features\n",
    "        feats = self._geo_feats(raw_m1[idx_slice], raw_m2[idx_slice], conf['pix_cm'])\n",
    "        \n",
    "        # 4. Targets\n",
    "        target = torch.zeros((self.local_window, NUM_CLASSES), dtype=torch.float32)\n",
    "        weights = torch.zeros(self.local_window, dtype=torch.float32)\n",
    "        \n",
    "        # Pad\n",
    "        if len(feats) < self.local_window:\n",
    "            pad_n = self.local_window - len(feats)\n",
    "            pad_f = np.zeros((pad_n, 11, 16), dtype=np.float32)\n",
    "            feats = np.concatenate([feats, pad_f], axis=0)\n",
    "            # Weights stay 0 at end\n",
    "            weights[:len(idx_slice)] = 1.0\n",
    "        else:\n",
    "            weights[:] = 1.0\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            ap = self.annot_dir / lab / f\"{sample['video_id']}.parquet\"\n",
    "            if ap.exists():\n",
    "                try:\n",
    "                    adf = pd.read_parquet(ap)\n",
    "                    for _, row in adf.iterrows():\n",
    "                        if row['action'] in ACTION_TO_IDX:\n",
    "                            st, et = int(row['start_frame'])-s, int(row['stop_frame'])-s\n",
    "                            st, et = max(0, st), min(self.local_window, et)\n",
    "                            if st < et: target[st:et, ACTION_TO_IDX[row['action']]] = 1.0\n",
    "                except: pass\n",
    "        \n",
    "        lab_idx = list(LAB_CONFIGS.keys()).index(lab) if lab in LAB_CONFIGS else 0\n",
    "        return torch.tensor(feats), torch.tensor(feats), target, weights, lab_idx\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode=='train' and random.random() < 0.9 and len(self.action_windows)>0:\n",
    "            i, c = self.action_windows[random.randint(0, len(self.action_windows)-1)]\n",
    "            return self._load(i, c)\n",
    "        return self._load(idx)\n",
    "    \n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "def pad_collate_dual(batch):\n",
    "    gx, lx, t, w, lid = zip(*batch)\n",
    "    return torch.stack(gx), torch.stack(lx), torch.stack(t), torch.stack(w), torch.tensor(lid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f641a2c1",
   "metadata": {
    "id": "15H3yhZEOh1Y",
    "papermill": {
     "duration": 0.007539,
     "end_time": "2025-12-09T11:34:22.075292",
     "exception": false,
     "start_time": "2025-12-09T11:34:22.067753",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Module 2: The Morphological & Interaction Core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58c7b9f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T11:34:22.090603Z",
     "iopub.status.busy": "2025-12-09T11:34:22.090300Z",
     "iopub.status.idle": "2025-12-09T11:34:22.102329Z",
     "shell.execute_reply": "2025-12-09T11:34:22.101732Z"
    },
    "id": "xsGFH_FLOh1Z",
    "papermill": {
     "duration": 0.020965,
     "end_time": "2025-12-09T11:34:22.103306",
     "exception": false,
     "start_time": "2025-12-09T11:34:22.082341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CANONICAL GRAPH ADAPTER (Signal Refinement)\n",
    "# ==============================================================================\n",
    "class CanonicalGraphAdapter(nn.Module):\n",
    "    # INPUT: [B, T, 11, 16] (Geometric Features)\n",
    "    def __init__(self, input_nodes=11, canonical_nodes=11, feat_dim=16, num_labs=20):\n",
    "        super().__init__()\n",
    "\n",
    "        # Learnable Projection Matrix: (NumLabs, 11, 11)\n",
    "        # Learns to map tracking artifacts to a canonical topology per lab\n",
    "        self.projection = nn.Parameter(torch.eye(input_nodes).unsqueeze(0).repeat(num_labs, 1, 1))\n",
    "        \n",
    "        # Identity initialization with slight noise\n",
    "        self.projection.data += torch.randn_like(self.projection) * 0.01\n",
    "\n",
    "        # Lab-Specific Bias (Correction for systematic sensor offset)\n",
    "        self.bias = nn.Parameter(torch.zeros(num_labs, 1, canonical_nodes, feat_dim))\n",
    "\n",
    "        # Refinement MLP (Cleans physics calculations)\n",
    "        self.refine = nn.Sequential(\n",
    "            nn.Linear(feat_dim, feat_dim * 2),\n",
    "            nn.LayerNorm(feat_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(feat_dim * 2, feat_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, lab_idx):\n",
    "        # x: (Batch, Time, 11, 16)\n",
    "        # lab_idx: (Batch)\n",
    "        b, t, n, f = x.shape\n",
    "\n",
    "        # 1. Fetch Weights\n",
    "        W = self.projection[lab_idx] # (B, 11, 11)\n",
    "        B = self.bias[lab_idx]       # (B, 1, 11, 16)\n",
    "\n",
    "        # 2. Graph Projection (Node Mixing)\n",
    "        # We process all time-steps in parallel by flattening B*T\n",
    "        x_flat = x.view(-1, n, f) # (B*T, 11, 16)\n",
    "        \n",
    "        # Prepare Projection Matrix: Expand to T, then view as (B*T, 11, 11)\n",
    "        W_flat = W.unsqueeze(1).repeat(1, t, 1, 1).view(-1, n, n)\n",
    "\n",
    "        # Apply Graph Projection: nodes^T * W\n",
    "        # (B*T, 16, 11) @ (B*T, 11, 11) -> (B*T, 16, 11)\n",
    "        x_t = x_flat.transpose(1, 2) \n",
    "        out = torch.bmm(x_t, W_flat) \n",
    "\n",
    "        # 3. Reshape Back & Apply Physics Refinement\n",
    "        out = out.transpose(1, 2).view(b, t, n, f)\n",
    "        out = out + B # Apply Bias\n",
    "        out = self.refine(out)\n",
    "\n",
    "        return out # (Batch, Time, 11, 16)\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. SOCIAL INTERACTION BLOCK (Updated for Geo-Features)\n",
    "# ==============================================================================\n",
    "class SocialInteractionBlock(nn.Module):\n",
    "    def __init__(self, node_dim=16, hidden_dim=64):\n",
    "        super().__init__()\n",
    "\n",
    "        # Relational MLP\n",
    "        # Takes the pre-calc geometric relations from Module 1\n",
    "        # [Rel_X, Rel_Y, Rel_Dist] + [Speed_Self, Speed_Other] (Derived)\n",
    "        self.relational_mlp = nn.Sequential(\n",
    "            nn.Linear(5, 32),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "\n",
    "        self.fusion = nn.Linear(node_dim * 2 + 16, hidden_dim)\n",
    "\n",
    "    def forward(self, agent_canon, target_canon):\n",
    "        # Input: [B, T, 11, 16] (Normalized Egocentric Features)\n",
    "        \n",
    "        # New Feature Map (Module 1):\n",
    "        # 0: PosX, 1: PosY (Self)\n",
    "        # 2: VelX, 3: VelY\n",
    "        # 4: Neighbor PosX, 5: Neighbor PosY (Explicit Relation)\n",
    "        # 6: Neighbor Dist\n",
    "        \n",
    "        # We extract Interaction Context from Node 0 (Body/Nose or Main Axis)\n",
    "        # or aggregate across nodes. Here we take the mean interaction \n",
    "        # features across all nodes for stability.\n",
    "        \n",
    "        # 1. Extract Interaction Features (Ch 4, 5, 6)\n",
    "        # Shape: [B, T, 3] (Mean over nodes)\n",
    "        interaction_raw = agent_canon[..., 4:7].mean(dim=2) \n",
    "        \n",
    "        # 2. Extract Dynamic Differences\n",
    "        # Speed is typically computed in loader, but let's take velocity diffs (Ch 2,3)\n",
    "        # Vel Self (Ch 2,3)\n",
    "        vel_self = agent_canon[..., 2:4].mean(dim=2) \n",
    "        # Vel Other (Inferred/Proxy via target tensor)\n",
    "        vel_targ = target_canon[..., 2:4].mean(dim=2)\n",
    "        \n",
    "        speed_diff = torch.norm(vel_self - vel_targ, dim=-1, keepdim=True)\n",
    "        dot_prod = (vel_self * vel_targ).sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Combine: [Ix, Iy, Dist, SpeedDiff, VelDot] -> 5 Dims\n",
    "        rel_feats = torch.cat([interaction_raw, speed_diff, dot_prod], dim=-1)\n",
    "        \n",
    "        # Embed\n",
    "        rel_embed = self.relational_mlp(rel_feats) # [B, T, 16]\n",
    "\n",
    "        return agent_canon, target_canon, rel_embed\n",
    "\n",
    "# ==============================================================================\n",
    "# WRAPPER: MORPHOLOGICAL INTERACTION CORE\n",
    "# ==============================================================================\n",
    "class MorphologicalInteractionCore(nn.Module):\n",
    "    def __init__(self, num_labs=20):\n",
    "        super().__init__()\n",
    "        # Standard input 11 canonical nodes\n",
    "        self.adapter = CanonicalGraphAdapter(input_nodes=11, canonical_nodes=11, num_labs=num_labs)\n",
    "        self.interaction = SocialInteractionBlock()\n",
    "\n",
    "        # Fusion: (11 nodes * 16 features * 2 agents) + 16 relation = 368\n",
    "        self.frame_fusion = nn.Linear(368, 128)\n",
    "\n",
    "    def forward(self, agent_x, target_x, lab_idx):\n",
    "        # 1. Adapt Topology (Refine Physics/Geometry)\n",
    "        a_c = self.adapter(agent_x, lab_idx)\n",
    "        t_c = self.adapter(target_x, lab_idx)\n",
    "\n",
    "        # 2. Compute Social Relations\n",
    "        # This uses the specific relative features baked into Module 1\n",
    "        _, _, rel_embed = self.interaction(a_c, t_c)\n",
    "\n",
    "        # 3. Flatten for Transformer Input\n",
    "        b, t, n, f = a_c.shape\n",
    "        a_flat = a_c.view(b, t, -1)\n",
    "        t_flat = t_c.view(b, t, -1)\n",
    "\n",
    "        # 4. Dense Fusion\n",
    "        # Fuses Self(A) + Self(B) + Relationship\n",
    "        combined = torch.cat([a_flat, t_flat, rel_embed], dim=-1) # [B, T, 368]\n",
    "        out = self.frame_fusion(combined) # [B, T, 128]\n",
    "\n",
    "        # Returns: \n",
    "        # out -> The Fused Token (used for Global Context / Temporal processing)\n",
    "        # a_c, t_c -> The Canonical Skeletons (used for Physics Gating in Mod 5)\n",
    "        return out, a_c, t_c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7495a235",
   "metadata": {
    "id": "zmX6cExTOh1Z",
    "papermill": {
     "duration": 0.006867,
     "end_time": "2025-12-09T11:34:22.117037",
     "exception": false,
     "start_time": "2025-12-09T11:34:22.110170",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Module 3: The Split-Stream Interaction Block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9afbf355",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T11:34:22.131847Z",
     "iopub.status.busy": "2025-12-09T11:34:22.131658Z",
     "iopub.status.idle": "2025-12-09T11:34:22.139574Z",
     "shell.execute_reply": "2025-12-09T11:34:22.139053Z"
    },
    "id": "je8yED4gOh1a",
    "papermill": {
     "duration": 0.01668,
     "end_time": "2025-12-09T11:34:22.140566",
     "exception": false,
     "start_time": "2025-12-09T11:34:22.123886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SplitStreamInteractionBlock(nn.Module):\n",
    "    def __init__(self, node_dim=16, hidden_dim=128):\n",
    "        super(SplitStreamInteractionBlock, self).__init__()\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "        # BRANCH A: SELF-BEHAVIOR STREAM (The \"Me\" Branch)\n",
    "        # ----------------------------------------------------------------------\n",
    "        # Focus: Posture, Grooming, Rearing, Running.\n",
    "        # Input: Strictly LIMITED to the first 4 channels of the Agent (Pos X/Y, Vel, Speed).\n",
    "        # We explicitly block Neighbor information (Channels 4+) from this stream\n",
    "        # to prevent \"Soft Leaks\" (the Self branch learning Pair behaviors).\n",
    "        self.self_input_size = 11 * 4 # 11 Nodes * 4 Feats (Pos/Vel)\n",
    "        \n",
    "        self.self_projector = nn.Sequential(\n",
    "            nn.Linear(self.self_input_size, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "        # BRANCH B: PAIR-BEHAVIOR STREAM (The \"Us\" Branch)\n",
    "        # ----------------------------------------------------------------------\n",
    "        # Focus: Interaction, Distance, Chasing, Fight.\n",
    "        # Input: Agent (Full) + Target (Full) + Interaction Token.\n",
    "        \n",
    "        # 1. Relational Engine\n",
    "        # The new Module 1 (Geo Features) pre-calculates distance/rel_pos in Ch 4-6.\n",
    "        # We extract this directly rather than re-calculating on the fly.\n",
    "        self.relational_mlp = nn.Sequential(\n",
    "            nn.Linear(3, 32), # [Rel_X, Rel_Y, Rel_Dist] averaged over nodes\n",
    "            nn.GELU(),\n",
    "            nn.Linear(32, 32)\n",
    "        )\n",
    "\n",
    "        # 2. Fusion Layer\n",
    "        # Agent (176) + Target (176) + Rel (32) + Roles (2)\n",
    "        full_node_dim = 11 * node_dim # 176\n",
    "        pair_input_dim = (full_node_dim * 2) + 32 + 2\n",
    "        \n",
    "        self.pair_projector = nn.Sequential(\n",
    "            nn.Linear(pair_input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        # Role Tokens (Solves \"Multi-Agent Roles\")\n",
    "        # [1, 0] = \"I am Acting\", [0, 1] = \"I am Receiving\"\n",
    "        self.role_embedding = nn.Parameter(torch.tensor([[1.0, 0.0], [0.0, 1.0]]))\n",
    "\n",
    "    def forward(self, agent_c, target_c):\n",
    "        \"\"\"\n",
    "        agent_c:  [Batch, Time, 11, 16] (Canonical Skeleton w/ Geo Features)\n",
    "        target_c: [Batch, Time, 11, 16] \n",
    "        \"\"\"\n",
    "        batch, time, nodes, feat = agent_c.shape\n",
    "\n",
    "        # ----------------------------------------------------------\n",
    "        # 1. PROCESS SELF STREAM (Strict Slicing)\n",
    "        # ----------------------------------------------------------\n",
    "        # Only take Channels 0,1,2,3 (Pos, Vel). \n",
    "        # Channels 4+ contain Neighbor Relative info -> BLOCKED.\n",
    "        agent_proprioception = agent_c[..., 0:4] # [B, T, 11, 4]\n",
    "        agent_flat_self = agent_proprioception.contiguous().view(batch, time, -1)\n",
    "        \n",
    "        self_feat = self.self_projector(agent_flat_self) # [B, T, 128]\n",
    "\n",
    "        # ----------------------------------------------------------\n",
    "        # 2. PROCESS PAIR STREAM (Full Context)\n",
    "        # ----------------------------------------------------------\n",
    "        # Flatten full skeletons\n",
    "        agent_flat_full = agent_c.view(batch, time, -1)\n",
    "        target_flat_full = target_c.view(batch, time, -1)\n",
    "        \n",
    "        # Extract Relational Data baked into Module 1 output\n",
    "        # Channels: 4 (Neighbor X), 5 (Neighbor Y), 6 (Dist)\n",
    "        # We assume mean interaction across nodes represents the body-level interaction\n",
    "        rel_feats = agent_c[..., 4:7].mean(dim=2) # [B, T, 3]\n",
    "        \n",
    "        # Embed Relation\n",
    "        rel_embed = self.relational_mlp(rel_feats) # [B, T, 32]\n",
    "\n",
    "        # Add Role Tokens (Broadcasting Agent Role [1,0])\n",
    "        role_token = self.role_embedding[0].view(1, 1, 2).expand(batch, time, 2)\n",
    "\n",
    "        # Fuse Pair Features\n",
    "        # Concatenate: Agent(Full) + Target(Full) + Relation + Role\n",
    "        pair_input = torch.cat([\n",
    "            agent_flat_full, \n",
    "            target_flat_full, \n",
    "            rel_embed, \n",
    "            role_token\n",
    "        ], dim=-1)\n",
    "        \n",
    "        pair_feat = self.pair_projector(pair_input) # [B, T, 128]\n",
    "\n",
    "        return self_feat, pair_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442edbd7",
   "metadata": {
    "id": "-TztXXo9Oh1a",
    "papermill": {
     "duration": 0.006669,
     "end_time": "2025-12-09T11:34:22.154112",
     "exception": false,
     "start_time": "2025-12-09T11:34:22.147443",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Module 4: The Local-Global Chronos Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de0eb665",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T11:34:22.169464Z",
     "iopub.status.busy": "2025-12-09T11:34:22.169195Z",
     "iopub.status.idle": "2025-12-09T11:34:22.182489Z",
     "shell.execute_reply": "2025-12-09T11:34:22.181947Z"
    },
    "id": "jhhoYs3JOh1a",
    "papermill": {
     "duration": 0.022404,
     "end_time": "2025-12-09T11:34:22.183469",
     "exception": false,
     "start_time": "2025-12-09T11:34:22.161065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class LocalGlobalChronosEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=128, hidden_dim=128):\n",
    "        super(LocalGlobalChronosEncoder, self).__init__()\n",
    "\n",
    "        # ======================================================================\n",
    "        # 1. GLOBAL CONTEXT STREAM (The \"Narrative\" Memory)\n",
    "        # ======================================================================\n",
    "        # Processes the 1 FPS Global Pair Features.\n",
    "        # Captures long-term states (e.g., \"Dominance established 10 mins ago\").\n",
    "        self.global_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        self.pos_encoder = PositionalEncoding(hidden_dim, max_len=5000)\n",
    "\n",
    "        global_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=4,\n",
    "            dim_feedforward=512,\n",
    "            batch_first=True,\n",
    "            dropout=0.1,\n",
    "            activation=\"gelu\"\n",
    "        )\n",
    "        self.global_transformer = nn.TransformerEncoder(global_layer, num_layers=2)\n",
    "\n",
    "        # ======================================================================\n",
    "        # 2. LOCAL SELF STREAM (The \"Me\" Branch) - DEEP TCN\n",
    "        # ======================================================================\n",
    "        # Updated: Receptive Field ~2.0 seconds (64 frames)\n",
    "        self.self_tcn = nn.Sequential(\n",
    "            # Frame Level (d=1)\n",
    "            nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            \n",
    "            # Short Range (d=2)\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=2, dilation=2),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            \n",
    "            # Medium Range (d=4)\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=4, dilation=4),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU(),\n",
    "\n",
    "            # Long Range (d=8) -> +16 frames context\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=8, dilation=8),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU(),\n",
    "\n",
    "            # Very Long Range (d=16) -> +32 frames context (TOTAL ~64)\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=16, dilation=16),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # Cross-Attention to Global \n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=4, batch_first=True)\n",
    "        self.self_norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        # ======================================================================\n",
    "        # 3. LOCAL PAIR STREAM (The \"Us\" Branch) - DEEP TCN\n",
    "        # ======================================================================\n",
    "        self.pair_tcn = nn.Sequential(\n",
    "            # Frame Level\n",
    "            nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            \n",
    "            # Short\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=2, dilation=2),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            \n",
    "            # Medium\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=4, dilation=4),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            \n",
    "            # Long (Interaction Buildup)\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=8, dilation=8),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            \n",
    "            # Very Long (Sustained Aggression/Chase)\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=16, dilation=16),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # Cross-Attention to Global\n",
    "        self.pair_attn = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=4, batch_first=True)\n",
    "        self.pair_norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, global_feat, local_self, local_pair):\n",
    "        \"\"\"\n",
    "        global_feat: [Batch, T_g, 128] \n",
    "        local_self:  [Batch, T_l, 128] \n",
    "        local_pair:  [Batch, T_l, 128] \n",
    "        \"\"\"\n",
    "\n",
    "        # --- A. Build Global Memory Bank ---\n",
    "        g_emb = self.global_proj(global_feat)\n",
    "        g_emb = self.pos_encoder(g_emb)\n",
    "        global_memory = self.global_transformer(g_emb) # [B, T_g, 128]\n",
    "\n",
    "        # --- B. Process Local Self Stream ---\n",
    "        # 1. TCN \n",
    "        s_in = local_self.permute(0, 2, 1) # [B, C, T]\n",
    "        s_tcn = self.self_tcn(s_in).permute(0, 2, 1) # [B, T, C]\n",
    "\n",
    "        # 2. Cross-Attention\n",
    "        # Query: Local TCN, Key/Value: Global Memory\n",
    "        s_ctx, _ = self.self_attn(query=s_tcn, key=global_memory, value=global_memory)\n",
    "        self_out = self.self_norm(s_tcn + s_ctx) \n",
    "\n",
    "        # --- C. Process Local Pair Stream ---\n",
    "        # 1. TCN\n",
    "        p_in = local_pair.permute(0, 2, 1)\n",
    "        p_tcn = self.pair_tcn(p_in).permute(0, 2, 1)\n",
    "\n",
    "        # 2. Cross-Attention\n",
    "        p_ctx, _ = self.pair_attn(query=p_tcn, key=global_memory, value=global_memory)\n",
    "        pair_out = self.pair_norm(p_tcn + p_ctx)\n",
    "\n",
    "        return self_out, pair_out\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=10000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        L = x.size(1)\n",
    "        if L > self.pe.size(0):\n",
    "            return x + self.pe[:self.pe.size(0), :].repeat(math.ceil(L/self.pe.size(0)), 1)[:L, :]\n",
    "        return x + self.pe[:L, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927ec100",
   "metadata": {
    "id": "gCgdUfDjOh1b",
    "papermill": {
     "duration": 0.006783,
     "end_time": "2025-12-09T11:34:22.197042",
     "exception": false,
     "start_time": "2025-12-09T11:34:22.190259",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Module 5: The Multi-Task Logic Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "582446e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T11:34:22.212104Z",
     "iopub.status.busy": "2025-12-09T11:34:22.211636Z",
     "iopub.status.idle": "2025-12-09T11:34:22.220000Z",
     "shell.execute_reply": "2025-12-09T11:34:22.219476Z"
    },
    "id": "DpqbgVe-Oh1b",
    "papermill": {
     "duration": 0.017029,
     "end_time": "2025-12-09T11:34:22.220945",
     "exception": false,
     "start_time": "2025-12-09T11:34:22.203916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiTaskLogicHead(nn.Module):\n",
    "    def __init__(self, input_dim=128, num_labs=20):\n",
    "        super(MultiTaskLogicHead, self).__init__()\n",
    "\n",
    "        # 1. DOMAIN EMBEDDING\n",
    "        self.lab_embedding = nn.Embedding(num_labs, 32)\n",
    "\n",
    "        # 2. FEATURE EXPANSION\n",
    "        fusion_dim = input_dim + 32\n",
    "        expanded_dim = 256 \n",
    "\n",
    "        # 3. HEAD A: SELF BEHAVIORS\n",
    "        self.self_classifier = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, expanded_dim),\n",
    "            nn.LayerNorm(expanded_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(expanded_dim, 11) \n",
    "        )\n",
    "\n",
    "        # 4. HEAD B: PAIR BEHAVIORS\n",
    "        self.pair_classifier = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, expanded_dim),\n",
    "            nn.LayerNorm(expanded_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(expanded_dim, 26) \n",
    "        )\n",
    "\n",
    "        # 5. CENTER REGRESSOR\n",
    "        self.center_regressor = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(64, 1) \n",
    "        )\n",
    "\n",
    "        # 6. PHYSICS LOGIC GATE \n",
    "        self.gate_control = nn.Linear(1, 1)\n",
    "        \n",
    "        # --- CRITICAL FIX: INITIALIZATION ---\n",
    "        with torch.no_grad():\n",
    "            # Force Gate Open (start unbiased)\n",
    "            self.gate_control.bias.fill_(2.0)\n",
    "            \n",
    "            # FORCE CLASSIFIERS TO PREDICT BACKGROUND (Prob ~0.01)\n",
    "            # The final Linear layer is at index [3] of Sequential\n",
    "            # Logits = -4.59 -> Sigmoid(-4.59) = 0.01\n",
    "            nn.init.constant_(self.self_classifier[3].bias, -4.59)\n",
    "            nn.init.constant_(self.pair_classifier[3].bias, -4.59)\n",
    "            \n",
    "            # Start Center Regression at 0.5 (Midpoint)\n",
    "            nn.init.constant_(self.center_regressor[2].bias, 0.0)\n",
    "\n",
    "    def forward(self, self_feat, pair_feat, lab_idx, agent_c, target_c):\n",
    "        \"\"\"\n",
    "        self_probs: [B, T, 11] (0.0 - 1.0)\n",
    "        pair_probs: [B, T, 26] (0.0 - 1.0)\n",
    "        \"\"\"\n",
    "        batch, time, _ = self_feat.shape\n",
    "\n",
    "        # A. Context\n",
    "        lab_context = self.lab_embedding(lab_idx).unsqueeze(1).expand(-1, time, -1)\n",
    "        self_input = torch.cat([self_feat, lab_context], dim=-1)\n",
    "        pair_input = torch.cat([pair_feat, lab_context], dim=-1)\n",
    "\n",
    "        # B. Raw Logits\n",
    "        self_logits = self.self_classifier(self_input) \n",
    "        pair_logits = self.pair_classifier(pair_input) \n",
    "        \n",
    "        # Center Score\n",
    "        center_score = torch.sigmoid(self.center_regressor(pair_input))\n",
    "\n",
    "        # C. Physics Gate\n",
    "        # Dist Logic\n",
    "        a_pos = agent_c[:, :, 0, :2]\n",
    "        t_pos = target_c[:, :, 0, :2]\n",
    "        dist = torch.norm(a_pos - t_pos, dim=-1, keepdim=True) \n",
    "\n",
    "        gate = torch.sigmoid(self.gate_control(dist))\n",
    "\n",
    "        # D. Activation\n",
    "        self_probs = torch.sigmoid(self_logits)\n",
    "        \n",
    "        # Combine Pair Logits with Gate\n",
    "        pair_probs = torch.sigmoid(pair_logits) * gate\n",
    "\n",
    "        return self_probs, pair_probs, center_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74767b59",
   "metadata": {
    "id": "zpQcsEIpOh1b",
    "papermill": {
     "duration": 0.006704,
     "end_time": "2025-12-09T11:34:22.234447",
     "exception": false,
     "start_time": "2025-12-09T11:34:22.227743",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Module 6: Final Assembly (EthoSwarmNet V3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c48f2825",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T11:34:22.248916Z",
     "iopub.status.busy": "2025-12-09T11:34:22.248710Z",
     "iopub.status.idle": "2025-12-09T11:34:22.258418Z",
     "shell.execute_reply": "2025-12-09T11:34:22.257720Z"
    },
    "id": "B_ZbjAuwOh1b",
    "papermill": {
     "duration": 0.018296,
     "end_time": "2025-12-09T11:34:22.259443",
     "exception": false,
     "start_time": "2025-12-09T11:34:22.241147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ==============================================================================\n",
    "# BEHAVIOR DEFINITIONS (For Output Stitching)\n",
    "# ==============================================================================\n",
    "# All 37 actions sorted alphabetically (Competition Standard)\n",
    "ACTION_LIST = sorted([\n",
    "    \"allogroom\", \"approach\", \"attack\", \"attemptmount\", \"avoid\", \"biteobject\",\n",
    "    \"chase\", \"chaseattack\", \"climb\", \"defend\", \"dig\", \"disengage\", \"dominance\",\n",
    "    \"dominancegroom\", \"dominancemount\", \"ejaculate\", \"escape\", \"exploreobject\",\n",
    "    \"flinch\", \"follow\", \"freeze\", \"genitalgroom\", \"huddle\", \"intromit\", \"mount\",\n",
    "    \"rear\", \"reciprocalsniff\", \"rest\", \"run\", \"selfgroom\", \"shepherd\", \"sniff\",\n",
    "    \"sniffbody\", \"sniffface\", \"sniffgenital\", \"submit\", \"tussle\"\n",
    "])\n",
    "\n",
    "# Subset: 11 Self Behaviors (Agent only)\n",
    "SELF_BEHAVIORS = sorted([\n",
    "    \"biteobject\", \"climb\", \"dig\", \"exploreobject\", \"freeze\", \"genitalgroom\",\n",
    "    \"huddle\", \"rear\", \"rest\", \"run\", \"selfgroom\"\n",
    "])\n",
    "\n",
    "# Subset: 26 Pair Behaviors (Agent + Target)\n",
    "PAIR_BEHAVIORS = sorted([\n",
    "    \"allogroom\", \"approach\", \"attack\", \"attemptmount\", \"avoid\", \"chase\",\n",
    "    \"chaseattack\", \"defend\", \"disengage\", \"dominance\", \"dominancegroom\",\n",
    "    \"dominancemount\", \"ejaculate\", \"escape\", \"flinch\", \"follow\", \"intromit\",\n",
    "    \"mount\", \"reciprocalsniff\", \"shepherd\", \"sniff\", \"sniffbody\", \"sniffface\",\n",
    "    \"sniffgenital\", \"submit\", \"tussle\"\n",
    "])\n",
    "\n",
    "class EthoSwarmNet(nn.Module):\n",
    "    def __init__(self, num_classes=37, input_dim=128):\n",
    "        super(EthoSwarmNet, self).__init__()\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "        # 1. Morphological Core (Module 2)\n",
    "        # ----------------------------------------------------------------------\n",
    "        self.morph_core = MorphologicalInteractionCore(num_labs=20)\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "        # 2. Split-Stream Block (Module 3)\n",
    "        # ----------------------------------------------------------------------\n",
    "        self.split_interaction = SplitStreamInteractionBlock(hidden_dim=128)\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "        # 3. Local-Global Chronos (Module 4)\n",
    "        # ----------------------------------------------------------------------\n",
    "        self.chronos = LocalGlobalChronosEncoder(input_dim=128, hidden_dim=128)\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "        # 4. Multi-Task Logic Head (Module 5)\n",
    "        # ----------------------------------------------------------------------\n",
    "        self.logic_head = MultiTaskLogicHead(\n",
    "            input_dim=128,\n",
    "            num_labs=20\n",
    "        )\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "        # 5. Output Stitching Maps\n",
    "        # ----------------------------------------------------------------------\n",
    "        self.register_buffer('self_indices', self._get_indices(SELF_BEHAVIORS))\n",
    "        self.register_buffer('pair_indices', self._get_indices(PAIR_BEHAVIORS))\n",
    "\n",
    "    def _get_indices(self, subset_list):\n",
    "        indices = []\n",
    "        for beh in subset_list:\n",
    "            try:\n",
    "                indices.append(ACTION_LIST.index(beh))\n",
    "            except ValueError:\n",
    "                pass\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def forward(self, global_agent, global_target, local_agent, local_target, lab_idx):\n",
    "        \"\"\"\n",
    "        The V3 Forward Pass:\n",
    "        Global/Local Streams -> Topology -> Split -> Time -> Logic -> Stitch\n",
    "        \"\"\"\n",
    "\n",
    "        # --- A. TOPOLOGY (Module 2) ---\n",
    "        g_out, _, _ = self.morph_core(global_agent, global_target, lab_idx)\n",
    "        _, l_ac, l_tc = self.morph_core(local_agent, local_target, lab_idx)\n",
    "\n",
    "        # --- B. SPLIT-STREAM (Module 3) ---\n",
    "        l_self, l_pair = self.split_interaction(l_ac, l_tc)\n",
    "\n",
    "        # --- C. TIME & CONTEXT (Module 4) ---\n",
    "        t_self, t_pair = self.chronos(g_out, l_self, l_pair)\n",
    "\n",
    "        # --- D. LOGIC & PHYSICS (Module 5) ---\n",
    "        # FIX: Now accepts 3 return values\n",
    "        # center_score is the Regression Head output (0.0 to 1.0)\n",
    "        p_self, p_pair, center_score = self.logic_head(t_self, t_pair, lab_idx, l_ac, l_tc)\n",
    "\n",
    "        # --- E. OUTPUT STITCHING ---\n",
    "        batch, time, _ = p_self.shape\n",
    "        # Reconstruct [Batch, T, 37] for classification targets\n",
    "        final_output = torch.zeros(batch, time, 37, device=p_self.device, dtype=p_self.dtype)\n",
    "\n",
    "        final_output.index_copy_(2, self.self_indices, p_self)\n",
    "        final_output.index_copy_(2, self.pair_indices, p_pair)\n",
    "        \n",
    "        # NOTE: For now, we are returning 'final_output' (37 classes) \n",
    "        # because the Training Loop expects [B, T, 37] matching targets.\n",
    "        # The 'center_score' improves internal gradient flow via backprop on Module 5.\n",
    "        # If you want to use Center Score explicitly in loss later, return it as tuple:\n",
    "        # return final_output, center_score\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2608126",
   "metadata": {
    "id": "NY0jnlvPOh1b",
    "papermill": {
     "duration": 0.006688,
     "end_time": "2025-12-09T11:34:22.272879",
     "exception": false,
     "start_time": "2025-12-09T11:34:22.266191",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Module 7: The Training Loop & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eda4cd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T11:34:22.287656Z",
     "iopub.status.busy": "2025-12-09T11:34:22.287452Z",
     "iopub.status.idle": "2025-12-09T11:34:22.512162Z",
     "shell.execute_reply": "2025-12-09T11:34:22.511409Z"
    },
    "id": "nze-CLBmOh1b",
    "outputId": "33d30aca-3c20-4fd9-986a-67f87511c3a1",
    "papermill": {
     "duration": 0.234065,
     "end_time": "2025-12-09T11:34:22.513698",
     "exception": false,
     "start_time": "2025-12-09T11:34:22.279633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ==============================================================================\n",
    "# UTILS & METRICS\n",
    "# ==============================================================================\n",
    "def load_lab_vocabulary(vocab_path, action_to_idx, num_classes, device):\n",
    "    \"\"\"\n",
    "    Loads a boolean mask [20, 37] where 1.0 means the lab annotates that action.\n",
    "    \"\"\"\n",
    "    # Default to \"Allow All\" if file missing\n",
    "    if not os.path.exists(vocab_path):\n",
    "        return torch.ones(25, 37).to(device)\n",
    "        \n",
    "    with open(vocab_path, 'r') as f:\n",
    "        vocab = json.load(f)\n",
    "    \n",
    "    # Must sort keys to match Module 1 index order\n",
    "    lab_names = sorted(list(LAB_CONFIGS.keys()))\n",
    "    mask = torch.zeros(len(lab_names), num_classes).to(device)\n",
    "    \n",
    "    for i, name in enumerate(lab_names):\n",
    "        if name in vocab:\n",
    "            for a in vocab[name]:\n",
    "                if a in action_to_idx: \n",
    "                    mask[i, action_to_idx[a]] = 1.0\n",
    "        else:\n",
    "            mask[i, :] = 1.0\n",
    "    return mask\n",
    "\n",
    "def get_batch_f1(probs_in, targets, batch_vocab_mask, temporal_weights):\n",
    "    \"\"\"\n",
    "    FIXED: Removed torch.sigmoid(). Input 'probs_in' is already 0.0-1.0 from Model.\n",
    "    \"\"\"\n",
    "    # 1. Binarize Predictions (probs are already 0-1)\n",
    "    preds = (probs_in > 0.4).float() \n",
    "    \n",
    "    # 2. Combine Masks\n",
    "    valid_pixels = temporal_weights.unsqueeze(-1) * batch_vocab_mask.unsqueeze(1)\n",
    "    \n",
    "    # 3. Calculate F1 only on VALID pixels\n",
    "    tp = (preds * targets * valid_pixels).sum()\n",
    "    fp = (preds * (1-targets) * valid_pixels).sum()\n",
    "    fn = ((1-preds) * targets * valid_pixels).sum()\n",
    "    \n",
    "    f1 = 2*tp / (2*tp + fp + fn + 1e-6)\n",
    "    return f1.item()\n",
    "\n",
    "# ==============================================================================\n",
    "# LOSS FUNCTION\n",
    "# ==============================================================================\n",
    "class DualStreamMaskedLoss(nn.Module):\n",
    "    def __init__(self, model_self_indices, model_pair_indices):\n",
    "        super().__init__()\n",
    "        self.self_idx = model_self_indices\n",
    "        self.pair_idx = model_pair_indices\n",
    "\n",
    "    def forward(self, model_output_probs, target, weight_mask, lab_vocab_mask):\n",
    "        \"\"\"\n",
    "        FIXED: Removed torch.sigmoid(). \n",
    "        Model outputs probabilities (0-1) due to Physics Gate.\n",
    "        \"\"\"\n",
    "        # Slice Output/Target\n",
    "        # Inputs are ALREADY PROBABILITIES\n",
    "        p_self = model_output_probs[:, :, self.self_idx]\n",
    "        p_pair = model_output_probs[:, :, self.pair_idx]\n",
    "        \n",
    "        t_self = target[:, :, self.self_idx]\n",
    "        t_pair = target[:, :, self.pair_idx]\n",
    "        \n",
    "        # Clamp for numerical stability (prevent log(0))\n",
    "        p_self = torch.clamp(p_self, 1e-7, 1 - 1e-7)\n",
    "        p_pair = torch.clamp(p_pair, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # Slice Lab Masks for Batch\n",
    "        m_self = lab_vocab_mask[:, self.self_idx].unsqueeze(1) # [B, 1, n_self]\n",
    "        m_pair = lab_vocab_mask[:, self.pair_idx].unsqueeze(1) # [B, 1, n_pair]\n",
    "        \n",
    "        # Temporal Mask [B, T, 1]\n",
    "        tm = weight_mask.unsqueeze(-1)\n",
    "        \n",
    "        # Compute Loss (Standard BCELoss, NOT WithLogits)\n",
    "        l_self_raw = F.binary_cross_entropy(p_self, t_self, reduction='none')\n",
    "        l_pair_raw = F.binary_cross_entropy(p_pair, t_pair, reduction='none')\n",
    "        \n",
    "        # Weighted Sum\n",
    "        loss_s = (l_self_raw * m_self * tm).sum() / ((m_self * tm).sum() + 1e-6)\n",
    "        loss_p = (l_pair_raw * m_pair * tm).sum() / ((m_pair * tm).sum() + 1e-6)\n",
    "        \n",
    "        return loss_s + loss_p\n",
    "\n",
    "# ==============================================================================\n",
    "# TRAINING CONTROLLER\n",
    "# ==============================================================================\n",
    "def train_ethoswarm_v3():\n",
    "    # --- 1. SETUP & PATHS ---\n",
    "    if 'mabe_mouse_behavior_detection_path' in globals():\n",
    "        DATA_PATH = globals()['mabe_mouse_behavior_detection_path']\n",
    "    elif os.path.exists('/kaggle/input/MABe-mouse-behavior-detection'):\n",
    "        DATA_PATH = '/kaggle/input/MABe-mouse-behavior-detection'\n",
    "    else: \n",
    "        print(\"Dataset not found.\"); return\n",
    "    \n",
    "    VOCAB_PATH = '/kaggle/input/mabe-metadata/results/lab_vocabulary.json'\n",
    "    \n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    BATCH_SIZE = 8 * max(1, gpu_count)\n",
    "    LEARNING_RATE = 3e-4 \n",
    "    NUM_EPOCHS = 3 \n",
    "\n",
    "    print(f\"Start Training on {gpu_count} GPU(s) | Batch Size: {BATCH_SIZE}\")\n",
    "\n",
    "    # --- 2. DATA PREP (Strict Video Split) ---\n",
    "    meta = pd.read_csv(f\"{DATA_PATH}/train.csv\")\n",
    "    vids = meta['video_id'].astype(str).unique()\n",
    "    np.random.shuffle(vids)\n",
    "    \n",
    "    split = int(len(vids) * 0.90)\n",
    "    train_ids = vids[:split]\n",
    "    val_ids = vids[split:]\n",
    "    \n",
    "    # Loaders - using Module 1 (Cached)\n",
    "    train_ds = BioPhysicsDataset(DATA_PATH, 'train', video_ids=train_ids)\n",
    "    val_ds = BioPhysicsDataset(DATA_PATH, 'train', video_ids=val_ids)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_collate_dual, num_workers=2)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=pad_collate_dual, num_workers=2)\n",
    "    \n",
    "    # --- 3. MODEL INITIALIZATION ---\n",
    "    model = EthoSwarmNet(num_classes=NUM_CLASSES, input_dim=128)\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    if gpu_count > 1:\n",
    "        print(f\"--> Activating Distributed Data Parallel on {gpu_count} GPUs\")\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=LEARNING_RATE, steps_per_epoch=len(train_loader), epochs=NUM_EPOCHS)\n",
    "    \n",
    "    # Load Masks\n",
    "    lab_masks = load_lab_vocabulary(VOCAB_PATH, ACTION_TO_IDX, NUM_CLASSES, DEVICE)\n",
    "    \n",
    "    self_indices = [ACTION_TO_IDX[a] for a in sorted(\n",
    "        [\"biteobject\", \"climb\", \"dig\", \"exploreobject\", \"freeze\", \"genitalgroom\", \n",
    "         \"huddle\", \"rear\", \"rest\", \"run\", \"selfgroom\"])]\n",
    "    pair_indices = [ACTION_TO_IDX[a] for a in sorted(\n",
    "        [\"allogroom\", \"approach\", \"attack\", \"attemptmount\", \"avoid\", \"chase\", \n",
    "         \"chaseattack\", \"defend\", \"disengage\", \"dominance\", \"dominancegroom\", \n",
    "         \"dominancemount\", \"ejaculate\", \"escape\", \"flinch\", \"follow\", \"intromit\", \n",
    "         \"mount\", \"reciprocalsniff\", \"shepherd\", \"sniff\", \"sniffbody\", \"sniffface\", \n",
    "         \"sniffgenital\", \"submit\", \"tussle\"])]\n",
    "         \n",
    "    loss_fn = DualStreamMaskedLoss(self_indices, pair_indices)\n",
    "\n",
    "    # --- 4. EPOCH LOOP ---\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        loop = tqdm(train_loader, desc=f\"Ep {epoch+1}\")\n",
    "        \n",
    "        run_loss = 0.0\n",
    "        run_f1 = 0.0\n",
    "        \n",
    "        for i, batch in enumerate(loop):\n",
    "            # Move 5 items to GPU\n",
    "            gx, lx, tgt, weights, lid = [b.to(DEVICE) for b in batch]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward \n",
    "            # Output is PROBABILITIES now (from Module 5+6)\n",
    "            probs = model(gx, gx, lx, lx, lid)\n",
    "            \n",
    "            # Loss Calc \n",
    "            loss = loss_fn(probs, tgt, weights, lab_masks[lid])\n",
    "            \n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Metrics\n",
    "            with torch.no_grad():\n",
    "                # FIXED: Don't sigmoid again\n",
    "                f1 = get_batch_f1(probs, tgt, lab_masks[lid], weights)\n",
    "                \n",
    "            run_loss = 0.9*run_loss + 0.1*loss.item() if i>0 else loss.item()\n",
    "            run_f1 = 0.9*run_f1 + 0.1*f1 if i>0 else f1\n",
    "            \n",
    "            if i % 20 == 0:\n",
    "                loop.set_postfix({'Loss': f\"{run_loss:.4f}\", 'F1': f\"{run_f1:.3f}\"})\n",
    "        \n",
    "        # Validation\n",
    "        print(\"Validating...\")\n",
    "        model.eval()\n",
    "        val_loss_sum = 0\n",
    "        batches = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                gx, lx, tgt, weights, lid = [b.to(DEVICE) for b in batch]\n",
    "                \n",
    "                probs = model(gx, gx, lx, lx, lid)\n",
    "                loss = loss_fn(probs, tgt, weights, lab_masks[lid])\n",
    "                \n",
    "                val_loss_sum += loss.item()\n",
    "                batches += 1\n",
    "                \n",
    "        print(f\"Val Loss: {val_loss_sum/batches:.4f}\")\n",
    "        \n",
    "        state = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\n",
    "        torch.save(state, f\"ethoswarm_v3_ep{epoch+1}.pth\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#     train_ethoswarm_v3()\n",
    "      pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3806e855",
   "metadata": {
    "papermill": {
     "duration": 0.00692,
     "end_time": "2025-12-09T11:34:22.527820",
     "exception": false,
     "start_time": "2025-12-09T11:34:22.520900",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Module 8 inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a9a3757",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T11:34:22.543011Z",
     "iopub.status.busy": "2025-12-09T11:34:22.542763Z",
     "iopub.status.idle": "2025-12-09T11:34:22.551631Z",
     "shell.execute_reply": "2025-12-09T11:34:22.551098Z"
    },
    "id": "S-eQVEZwOh1c",
    "papermill": {
     "duration": 0.018119,
     "end_time": "2025-12-09T11:34:22.552682",
     "exception": false,
     "start_time": "2025-12-09T11:34:22.534563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ==============================================================================\n",
    "# # MODULE 8: FINAL INFERENCE (CALIBRATED THRESHOLDS)\n",
    "# # ==============================================================================\n",
    "# import os\n",
    "# import gc\n",
    "# import glob\n",
    "# import ast\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from tqdm.auto import tqdm\n",
    "# from scipy.ndimage import median_filter\n",
    "# from pathlib import Path\n",
    "\n",
    "# # --- 1. CONFIGURATION ---\n",
    "# try:\n",
    "#     DEVICE\n",
    "# except NameError:\n",
    "#     DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# TEST_CSV = '/kaggle/input/MABe-mouse-behavior-detection/test.csv'\n",
    "# TEST_TRACKING = '/kaggle/input/MABe-mouse-behavior-detection/test_tracking'\n",
    "# SUBMISSION_PATH = 'submission.csv'\n",
    "\n",
    "# def find_weights(filename='ethoswarm_v3_ep3.pth'):\n",
    "#     files = glob.glob(f\"/kaggle/input/**/{filename}\", recursive=True)\n",
    "#     if files: return files[0]\n",
    "#     return filename \n",
    "\n",
    "# WEIGHTS_PATH = find_weights('ethoswarm_v3_ep3.pth')\n",
    "# INFERENCE_CHUNK_SIZE = 4000\n",
    "# GPU_BATCH_SIZE = 8\n",
    "# NUM_WORKERS = 0 \n",
    "\n",
    "# # --- 2. CALIBRATED THRESHOLDS ---\n",
    "# # Derived from Forensic Report: Threshold = ~0.4 * Max_Observed_Conf\n",
    "# TH = {\n",
    "#     \"attack\": 0.12,       # Max was 0.30\n",
    "#     \"rear\": 0.15,         # Max was 0.46\n",
    "#     \"sniff\": 0.05,        # Max was 0.13\n",
    "#     \"approach\": 0.05,     # Max was 0.14\n",
    "#     \"mount\": 0.06,        # Max was 0.14\n",
    "#     \"intromit\": 0.06,     # Max was 0.14\n",
    "#     \"chase\": 0.03,        # Max was 0.07 -> LOWERED\n",
    "#     \"escape\": 0.01,       # Max was 0.015 -> LOWERED\n",
    "#     \"submit\": 0.015,      # Max was 0.036 -> LOWERED (Critical Fix)\n",
    "#     \"avoid\": 0.04,        # Max was 0.10\n",
    "#     \"biteobject\": 0.02,   # Max was 0.04\n",
    "#     \"climb\": 0.02,        # Max was 0.05\n",
    "#     \"dominance\": 0.04,    # Max was 0.09\n",
    "#     \"tussle\": 0.01        # Max was 0.012\n",
    "# }\n",
    "# DEF_TH = 0.02 # Catch-all for very weak signals\n",
    "\n",
    "# BODY_PARTS = [\"ear_left\", \"ear_right\", \"nose\", \"neck\", \"body_center\",\"lateral_left\", \"lateral_right\", \"hip_left\", \"hip_right\",\"tail_base\", \"tail_tip\"]\n",
    "\n",
    "# ACTION_LIST = sorted([\n",
    "#     \"allogroom\", \"approach\", \"attack\", \"attemptmount\", \"avoid\", \"biteobject\",\n",
    "#     \"chase\", \"chaseattack\", \"climb\", \"defend\", \"dig\", \"disengage\", \"dominance\",\n",
    "#     \"dominancegroom\", \"dominancemount\", \"ejaculate\", \"escape\", \"exploreobject\",\n",
    "#     \"flinch\", \"follow\", \"freeze\", \"genitalgroom\", \"huddle\", \"intromit\", \"mount\",\n",
    "#     \"rear\", \"reciprocalsniff\", \"rest\", \"run\", \"selfgroom\", \"shepherd\", \"sniff\",\n",
    "#     \"sniffbody\", \"sniffface\", \"sniffgenital\", \"submit\", \"tussle\"\n",
    "# ])\n",
    "\n",
    "# # Separation for Masking\n",
    "# SELF_BEHAVIORS = sorted([\"biteobject\", \"climb\", \"dig\", \"exploreobject\", \"freeze\", \"genitalgroom\", \"huddle\", \"rear\", \"rest\", \"run\", \"selfgroom\"])\n",
    "# PAIR_BEHAVIORS = sorted(list(set(ACTION_LIST) - set(SELF_BEHAVIORS)))\n",
    "# SELF_IDXS = [ACTION_LIST.index(a) for a in SELF_BEHAVIORS]\n",
    "# PAIR_IDXS = [ACTION_LIST.index(a) for a in PAIR_BEHAVIORS]\n",
    "\n",
    "# LAB_CONFIGS = {\n",
    "#     \"AdaptableSnail\":       {\"thresh\": 718.59, \"window\": 120, \"pix_cm\": 14.5},\n",
    "#     \"DEFAULT\":              {\"thresh\": 150.00, \"window\": 128, \"pix_cm\": 15.0}\n",
    "# }\n",
    "# LAB_NAME_TO_IDX = {name: i for i, name in enumerate(sorted(LAB_CONFIGS.keys()))}\n",
    "\n",
    "# # ==============================================================================\n",
    "# # 8.1 PARSER\n",
    "# # ==============================================================================\n",
    "# def parse_test_tasks(test_csv_path):\n",
    "#     if not os.path.exists(test_csv_path): return []\n",
    "#     df = pd.read_csv(test_csv_path)\n",
    "#     tasks = []\n",
    "    \n",
    "#     print(f\"Scanning {len(df)} videos for tasks...\")\n",
    "#     for i, row in df.iterrows():\n",
    "#         vid = str(row['video_id'])\n",
    "#         lab = row['lab_id']\n",
    "#         raw = row.get('behaviors_labeled', None)\n",
    "#         try:\n",
    "#             if pd.isna(raw):\n",
    "#                 tasks.append({'vid': vid, 'lab': lab, 'ag': 'mouse1', 'tg': 'mouse2'})\n",
    "#                 tasks.append({'vid': vid, 'lab': lab, 'ag': 'mouse2', 'tg': 'mouse1'})\n",
    "#                 continue\n",
    "            \n",
    "#             labels = ast.literal_eval(raw)\n",
    "#             seen = set()\n",
    "#             for item in labels:\n",
    "#                 if isinstance(item, str): parts = item.split(',')\n",
    "#                 else: parts = item \n",
    "                \n",
    "#                 if len(parts) >= 2:\n",
    "#                     ag = parts[0].strip()\n",
    "#                     tg = parts[1].strip()\n",
    "#                     if tg.lower() == 'self' or tg == '': tg = 'self'\n",
    "                    \n",
    "#                     if (ag, tg) not in seen:\n",
    "#                         tasks.append({'vid': vid, 'lab': lab, 'ag': ag, 'tg': tg})\n",
    "#                         seen.add((ag, tg))\n",
    "#         except:\n",
    "#             tasks.append({'vid': vid, 'lab': lab, 'ag': 'mouse1', 'tg': 'mouse2'})\n",
    "            \n",
    "#     print(f\"Total Unique Tasks: {len(tasks)}\")\n",
    "#     return tasks\n",
    "\n",
    "# # ==============================================================================\n",
    "# # 8.2 DATASET\n",
    "# # ==============================================================================\n",
    "# class TaskInferenceDataset(Dataset):\n",
    "#     def __init__(self, tasks, tracking_dir):\n",
    "#         self.samples = tasks\n",
    "#         self.tracking_dir = Path(tracking_dir)\n",
    "#         self.bp_idx = {b:i for i,b in enumerate(BODY_PARTS)}\n",
    "#         self.ALIASES = {'hip_left': ['lateral_left', 'tail_base'], 'hip_right': ['lateral_right', 'tail_base'], 'neck': ['headpiece_bottombackright', 'nose'], 'body_center': ['tail_base']}\n",
    "\n",
    "#     def __len__(self): return len(self.samples)\n",
    "\n",
    "#     def _fix_teleport(self, pos):\n",
    "#         T, N, _ = pos.shape\n",
    "#         missing = (np.abs(pos).sum(axis=2) < 1e-6)\n",
    "#         cleaned = pos.copy()\n",
    "#         cols = np.where(missing.any(axis=0))[0]\n",
    "#         if len(cols) > 0:\n",
    "#             t = np.arange(T)\n",
    "#             for n in cols:\n",
    "#                 mask = missing[:, n]\n",
    "#                 if np.any(~mask):\n",
    "#                     cleaned[mask, n, 0] = np.interp(t[mask], t[~mask], pos[~mask, n, 0])\n",
    "#                     cleaned[mask, n, 1] = np.interp(t[mask], t[~mask], pos[~mask, n, 1])\n",
    "#         return cleaned\n",
    "\n",
    "#     def _geo_features(self, m1, m2, lab):\n",
    "#         # NORMALIZED SCALE\n",
    "#         conf = LAB_CONFIGS.get(lab, LAB_CONFIGS['DEFAULT'])\n",
    "#         px = float(conf['pix_cm']) + 1e-6\n",
    "#         m1 = m1.astype(np.float32) / px\n",
    "#         m2 = m2.astype(np.float32) / px\n",
    "        \n",
    "#         origin = m1[:, 9:10]; vec = m1[:, 3:4] - origin\n",
    "#         ang = np.arctan2(vec[..., 0], vec[..., 1]).flatten()\n",
    "#         c, s = np.cos(ang), np.sin(ang)\n",
    "        \n",
    "#         def rotate(p):\n",
    "#             p_c = p - origin\n",
    "#             px = p_c[...,0] * c[:,None] - p_c[...,1] * s[:,None]\n",
    "#             py = p_c[...,0] * s[:,None] + p_c[...,1] * c[:,None]\n",
    "#             return np.stack([px, py], axis=-1)\n",
    "\n",
    "#         # Rotate\n",
    "#         r_m1 = rotate(m1)\n",
    "#         r_m2 = rotate(m2) \n",
    "        \n",
    "#         rx, ry = r_m1[..., 0], r_m1[..., 1]\n",
    "#         ox, oy = r_m2[..., 0], r_m2[..., 1]\n",
    "        \n",
    "#         vel_x = np.diff(rx, axis=0, prepend=rx[0:1])\n",
    "#         vel_y = np.diff(ry, axis=0, prepend=ry[0:1])\n",
    "#         spd = np.sqrt(vel_x**2 + vel_y**2)\n",
    "#         dist = np.sqrt((rx - ox)**2 + (ry - oy)**2)\n",
    "        \n",
    "#         feats = np.stack([rx, ry, vel_x, vel_y, spd, dist, ox, oy], axis=-1)\n",
    "#         L, N, _ = feats.shape\n",
    "#         padded = np.zeros((L, N, 16), dtype=np.float32)\n",
    "#         padded[..., :8] = feats\n",
    "        \n",
    "#         padded = np.nan_to_num(padded, nan=0.0)\n",
    "#         padded = np.clip(padded, -30.0, 30.0) \n",
    "        \n",
    "#         return padded\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         job = self.samples[idx]\n",
    "#         job_out = job.copy()\n",
    "        \n",
    "#         try:\n",
    "#             p = self.tracking_dir / job['lab'] / f\"{job['vid']}.parquet\"\n",
    "#             if not p.exists(): return torch.zeros(1), 0, job_out\n",
    "\n",
    "#             df = pd.read_parquet(p)\n",
    "#             max_f = int(df.video_frame.max() + 1) if 'video_frame' in df else len(df)\n",
    "#             limit = min(max_f, 3000000)\n",
    "            \n",
    "#             uids = df.mouse_id.unique()\n",
    "#             def resolve(req):\n",
    "#                 if req == 'self': return None\n",
    "#                 for u in uids:\n",
    "#                     if str(u) == str(req): return u\n",
    "#                     if f\"mouse{u}\" == str(req): return u\n",
    "#                 return uids[0]\n",
    "\n",
    "#             real_ag = resolve(job['ag'])\n",
    "#             real_tg = resolve(job['tg'])\n",
    "            \n",
    "#             if real_tg is None:\n",
    "#                 real_tg = real_ag\n",
    "#                 job_out['is_self'] = True\n",
    "#             else:\n",
    "#                 job_out['is_self'] = False\n",
    "                \n",
    "#             job_out['sub_ag'] = f\"mouse{real_ag}\" if str(real_ag).isdigit() else str(real_ag)\n",
    "#             job_out['sub_tg'] = 'self' if job_out['is_self'] else (f\"mouse{real_tg}\" if str(real_tg).isdigit() else str(real_tg))\n",
    "\n",
    "#             raw_ag = np.zeros((limit, 11, 2), dtype=np.float32)\n",
    "#             raw_tg = np.zeros_like(raw_ag)\n",
    "#             avail = set(df['bodypart'].unique())\n",
    "            \n",
    "#             def extract(mid, dest):\n",
    "#                 m_rows = df[df.mouse_id == mid]\n",
    "#                 groups = dict(tuple(m_rows.groupby('bodypart')))\n",
    "#                 for bp, idx in self.bp_idx.items():\n",
    "#                     src = bp\n",
    "#                     if src not in avail and bp in self.ALIASES:\n",
    "#                         for a in self.ALIASES[bp]:\n",
    "#                             if a in avail: src = a; break\n",
    "#                     if src in groups:\n",
    "#                         g = groups[src]\n",
    "#                         f = g.video_frame.values.astype(int) if 'video_frame' in g else np.arange(len(g))\n",
    "#                         v = f < limit\n",
    "#                         dest[f[v], idx] = g[['x','y']].values[v]\n",
    "\n",
    "#             extract(real_ag, raw_ag)\n",
    "#             extract(real_tg, raw_tg)\n",
    "\n",
    "#             raw_ag = self._fix_teleport(raw_ag)\n",
    "#             raw_tg = self._fix_teleport(raw_tg)\n",
    "            \n",
    "#             for m in [raw_ag, raw_tg]:\n",
    "#                 tail_base = m[:, 9]\n",
    "#                 for i in [3, 4, 7, 8]: m[:, i] = tail_base if m[:, i].sum() == 0 else m[:, i]\n",
    "\n",
    "#             feats = self._geo_features(raw_ag, raw_tg, job['lab'])\n",
    "#             lid = LAB_NAME_TO_IDX.get(job['lab'], 0)\n",
    "            \n",
    "#             return torch.tensor(feats), lid, job_out\n",
    "\n",
    "#         except: return torch.zeros(1), 0, job_out\n",
    "\n",
    "# def collate_fn(b): return b[0]\n",
    "\n",
    "# # ==============================================================================\n",
    "# # 8.3 MAIN EXECUTION\n",
    "# # ==============================================================================\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(f\"--- STARTING MODULE 8: FINAL INFERENCE (CALIBRATED) ---\")\n",
    "    \n",
    "#     if not os.path.exists(TEST_CSV):\n",
    "#         pd.DataFrame(columns=['row_id','video_id','action']).to_csv(SUBMISSION_PATH, index=False); exit()\n",
    "\n",
    "#     tasks = parse_test_tasks(TEST_CSV)\n",
    "#     ds = TaskInferenceDataset(tasks, TEST_TRACKING)\n",
    "#     loader = DataLoader(ds, batch_size=1, collate_fn=collate_fn, num_workers=NUM_WORKERS)\n",
    "    \n",
    "#     try: model = EthoSwarmNet(37, 128).to(DEVICE)\n",
    "#     except: print(\"ERROR: EthoSwarmNet not found.\"); exit()\n",
    "\n",
    "#     if os.path.exists(WEIGHTS_PATH):\n",
    "#         try:\n",
    "#             model.load_state_dict(torch.load(WEIGHTS_PATH, map_location=DEVICE), strict=False)\n",
    "#             print(f\"Weights loaded from {WEIGHTS_PATH}\")\n",
    "#         except: pass\n",
    "#     model.eval()\n",
    "    \n",
    "#     first_pass = True\n",
    "#     with open(SUBMISSION_PATH, 'w') as f:\n",
    "#         f.write(\"row_id,video_id,agent_id,target_id,action,start_frame,stop_frame\\n\")\n",
    "#         row_id = 0\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             for feat, lid, job in tqdm(loader, desc=\"Inferring\"):\n",
    "#                 if feat.dim() < 3 or feat.shape[0] < 5: continue\n",
    "                \n",
    "#                 T = feat.shape[0]\n",
    "#                 rem = T % INFERENCE_CHUNK_SIZE\n",
    "#                 if rem > 0:\n",
    "#                     pad = INFERENCE_CHUNK_SIZE - rem\n",
    "#                     feat = torch.cat([feat, feat[-1:].repeat(pad,1,1)], 0)\n",
    "                \n",
    "#                 c_f = feat.view(-1, INFERENCE_CHUNK_SIZE, 11, 16)\n",
    "#                 preds_list = []\n",
    "#                 for i in range(len(c_f)):\n",
    "#                     ba = c_f[i:i+1].to(DEVICE)\n",
    "#                     l_idx = torch.tensor([lid], device=DEVICE)\n",
    "#                     out = model(ba, ba, ba, ba, l_idx)\n",
    "#                     preds_list.append(out.cpu())\n",
    "                \n",
    "#                 # --- RAW PROBABILITIES ---\n",
    "#                 probs = torch.cat(preds_list, 1).flatten(0, 1)[:T].numpy()\n",
    "#                 row_max = probs.max(axis=1, keepdims=True) + 1e-6\n",
    "#                 probs_norm = probs / row_max \n",
    "                \n",
    "#                 # --- DEBUG SNAPSHOT ---\n",
    "#                 if first_pass:\n",
    "#                     print(f\"\\n--- DEBUG SNAPSHOT ({job['vid']}) ---\")\n",
    "#                     print(f\"Task: {job['sub_ag']} -> {job['sub_tg']}\")\n",
    "#                     mid = T // 2\n",
    "#                     top3 = np.argsort(probs_norm[mid])[-3:][::-1]\n",
    "#                     print(f\"Pred (Norm): {[f'{ACTION_LIST[i]}({probs_norm[mid,i]:.3f})' for i in top3]}\")\n",
    "#                     first_pass = False\n",
    "                \n",
    "#                 aid, tid = job['sub_ag'], job['sub_tg']\n",
    "                \n",
    "#                 # --- STRICT MASKING ---\n",
    "#                 if job['is_self']:\n",
    "#                     probs_norm[:, PAIR_IDXS] = 0.0 \n",
    "#                 else:\n",
    "#                     probs_norm[:, SELF_IDXS] = 0.0\n",
    "                \n",
    "#                 # --- TOP-K GATING ---\n",
    "#                 topk_vals = np.sort(probs_norm, axis=1)[:, -3:] \n",
    "#                 min_k = topk_vals[:, 0:1] \n",
    "#                 mask_k = (probs_norm >= min_k).astype(np.float32)\n",
    "#                 probs_norm *= mask_k\n",
    "\n",
    "#                 for cls_idx, act in enumerate(ACTION_LIST):\n",
    "#                     trace = probs_norm[:, cls_idx]\n",
    "#                     if trace.max() < 0.01: continue\n",
    "                    \n",
    "#                     # Use Calibrated Threshold\n",
    "#                     thresh = TH.get(act, DEF_TH)\n",
    "                    \n",
    "#                     smoothed = median_filter(trace, size=7)\n",
    "#                     binary = (smoothed > thresh).astype(np.int8)\n",
    "                    \n",
    "#                     if binary.sum() == 0: continue\n",
    "#                     diffs = np.diff(np.concatenate(([0], binary, [0])))\n",
    "#                     starts = np.where(diffs == 1)[0]\n",
    "#                     ends = np.where(diffs == -1)[0]\n",
    "                    \n",
    "#                     for s, e in zip(starts, ends):\n",
    "#                         if (e - s) < 2: continue\n",
    "#                         f.write(f\"{row_id},{job['vid']},{aid},{tid},{act},{s},{e}\\n\")\n",
    "#                         row_id += 1\n",
    "                \n",
    "#                 del feat, c_f, probs, preds_list\n",
    "    \n",
    "#     print(f\"Success! Generated {row_id} rows.\")\n",
    "#     print(f\"Submission saved to {SUBMISSION_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e321d324",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T11:34:22.567565Z",
     "iopub.status.busy": "2025-12-09T11:34:22.567252Z",
     "iopub.status.idle": "2025-12-09T11:34:22.573648Z",
     "shell.execute_reply": "2025-12-09T11:34:22.572965Z"
    },
    "papermill": {
     "duration": 0.015122,
     "end_time": "2025-12-09T11:34:22.574734",
     "exception": false,
     "start_time": "2025-12-09T11:34:22.559612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ==============================================================================\n",
    "# # MODULE 9: FORENSIC VALIDATION (SCALE 1.0 ONLY)\n",
    "# # ==============================================================================\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import glob\n",
    "# import os\n",
    "# import random\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from pathlib import Path\n",
    "\n",
    "# # --- CONFIGURATION ---\n",
    "# LAB_ID = 'AdaptableSnail'\n",
    "# MODEL_PATH = '/kaggle/input/mabe-separated/ethoswarm_v3_ep3.pth'\n",
    "# TRACKING_PATH = '/kaggle/input/MABe-mouse-behavior-detection/train_tracking'\n",
    "# ANNOTATION_PATH = '/kaggle/input/MABe-mouse-behavior-detection/train_annotation'\n",
    "# SCALE = 1.0 # Locked per your request\n",
    "\n",
    "# # --- CONSTANTS ---\n",
    "# ACTION_LIST = sorted([\n",
    "#     \"allogroom\", \"approach\", \"attack\", \"attemptmount\", \"avoid\", \"biteobject\",\n",
    "#     \"chase\", \"chaseattack\", \"climb\", \"defend\", \"dig\", \"disengage\", \"dominance\",\n",
    "#     \"dominancegroom\", \"dominancemount\", \"ejaculate\", \"escape\", \"exploreobject\",\n",
    "#     \"flinch\", \"follow\", \"freeze\", \"genitalgroom\", \"huddle\", \"intromit\", \"mount\",\n",
    "#     \"rear\", \"reciprocalsniff\", \"rest\", \"run\", \"selfgroom\", \"shepherd\", \"sniff\",\n",
    "#     \"sniffbody\", \"sniffface\", \"sniffgenital\", \"submit\", \"tussle\"\n",
    "# ])\n",
    "# BODY_PARTS = [\"ear_left\", \"ear_right\", \"nose\", \"neck\", \"body_center\",\"lateral_left\", \"lateral_right\", \"hip_left\", \"hip_right\",\"tail_base\", \"tail_tip\"]\n",
    "# LAB_CONFIGS = {\"AdaptableSnail\": {\"thresh\": 718.59, \"window\": 120, \"pix_cm\": 14.5}, \"DEFAULT\": {\"thresh\": 150.0, \"window\": 128, \"pix_cm\": 15.0}}\n",
    "\n",
    "# # --- DATASET ---\n",
    "# class ValidationDataset(Dataset):\n",
    "#     def __init__(self, vid_id, lab_id, tracking_dir, agent_id, target_id):\n",
    "#         self.vid = vid_id; self.lab = lab_id; self.tracking_dir = Path(tracking_dir)\n",
    "#         self.agent_id = agent_id; self.target_id = target_id\n",
    "#         self.p_path = self.tracking_dir / self.lab / f\"{self.vid}.parquet\"\n",
    "#         self.ALIASES = {'hip_left': ['lateral_left', 'tail_base'], 'hip_right': ['lateral_right', 'tail_base'], 'neck': ['headpiece_bottombackright', 'nose'], 'body_center': ['tail_base']}\n",
    "#         self.bp_idx = {b:i for i,b in enumerate(BODY_PARTS)}\n",
    "\n",
    "#     def _geo_features(self, m1, m2, lab):\n",
    "#         conf = LAB_CONFIGS.get(lab, LAB_CONFIGS['DEFAULT'])\n",
    "#         px = float(conf['pix_cm']) + 1e-6\n",
    "#         m1 = m1 / px; m2 = m2 / px\n",
    "        \n",
    "#         origin = m1[:, 9:10]; vec = m1[:, 3:4] - origin\n",
    "#         ang = np.arctan2(vec[..., 0], vec[..., 1]).flatten(); c, s = np.cos(ang), np.sin(ang)\n",
    "#         def rotate(p):\n",
    "#             px = p[...,0] * c[:,None] - p[...,1] * s[:,None]\n",
    "#             py = p[...,0] * s[:,None] + p[...,1] * c[:,None]\n",
    "#             return np.stack([px, py], axis=-1)\n",
    "\n",
    "#         rot_m1 = rotate(m1 - origin)\n",
    "#         rot_m2 = rotate(m2 - origin)\n",
    "#         v1 = np.diff(rot_m1, axis=0, prepend=rot_m1[0:1])\n",
    "#         s1 = np.linalg.norm(v1, axis=-1, keepdims=True)\n",
    "#         v2 = np.diff(rot_m2, axis=0, prepend=rot_m2[0:1])\n",
    "#         s2 = np.linalg.norm(v2, axis=-1, keepdims=True)\n",
    "#         dist = np.linalg.norm(rot_m2, axis=-1, keepdims=True)\n",
    "        \n",
    "#         lid = 0; lab_token = np.full_like(s1, lid); zeros = np.zeros_like(s1)\n",
    "#         # RAW ALIGNMENT\n",
    "#         feat = np.concatenate([rot_m1, v1, rot_m2, v2, s1, s2, dist, lab_token, zeros, zeros, zeros, zeros], axis=-1)\n",
    "#         return feat.astype(np.float32)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         if not self.p_path.exists(): return None\n",
    "#         df = pd.read_parquet(self.p_path)\n",
    "#         limit = min(len(df), 150000) \n",
    "#         raw_m1 = np.zeros((limit, 11, 2), dtype=np.float32)\n",
    "#         raw_m2 = np.zeros_like(raw_m1)\n",
    "#         avail = set(df.bodypart.unique())\n",
    "#         uids = df.mouse_id.unique()\n",
    "        \n",
    "#         def resolve_id(req_id):\n",
    "#             if req_id in uids: return req_id\n",
    "#             for u in uids:\n",
    "#                 if str(u) == str(req_id): return u\n",
    "#                 if f\"mouse{u}\" == str(req_id): return u\n",
    "#             return uids[0]\n",
    "\n",
    "#         real_ag = resolve_id(self.agent_id)\n",
    "#         real_tg = resolve_id(self.target_id)\n",
    "\n",
    "#         def load_mouse(mid, dest):\n",
    "#             m_rows = df[df.mouse_id == mid]\n",
    "#             groups = dict(tuple(m_rows.groupby('bodypart')))\n",
    "#             for bp, idx in self.bp_idx.items():\n",
    "#                 src = bp\n",
    "#                 if src not in avail and bp in self.ALIASES:\n",
    "#                     for a in self.ALIASES[bp]:\n",
    "#                         if a in avail: src = a; break\n",
    "#                 if src in groups:\n",
    "#                     g = groups[src]\n",
    "#                     f = g.video_frame.values.astype(int) if 'video_frame' in g else np.arange(len(g))\n",
    "#                     v = f < limit\n",
    "#                     dest[f[v], idx] = g[['x','y']].values[v]\n",
    "\n",
    "#         load_mouse(real_ag, raw_m1)\n",
    "#         load_mouse(real_tg, raw_m2)\n",
    "#         for m in [raw_m1, raw_m2]:\n",
    "#             tail_base = m[:, 9]\n",
    "#             for i in [3, 4, 7, 8]: \n",
    "#                 if m[:, i].sum() == 0: m[:, i] = tail_base\n",
    "\n",
    "#         f_ag = self._geo_features(raw_m1, raw_m2, self.lab)\n",
    "#         f_tg = self._geo_features(raw_m2, raw_m1, self.lab)\n",
    "#         return torch.tensor(f_ag), torch.tensor(f_tg), 0\n",
    "\n",
    "#     def __len__(self): return 1\n",
    "\n",
    "# # --- MULTI-FILE SCANNER ---\n",
    "# def get_diverse_events():\n",
    "#     print(\"Scanning for events...\")\n",
    "#     annotation_files = glob.glob(f\"{ANNOTATION_PATH}/{LAB_ID}/*.parquet\")\n",
    "#     random.shuffle(annotation_files)\n",
    "#     events = []\n",
    "#     TARGETS = ['rear', 'chase', 'attack', 'sniff'] # Focused list\n",
    "#     found = {t:0 for t in TARGETS}\n",
    "    \n",
    "#     # Deep scan (100 files max)\n",
    "#     for p in annotation_files[:100]:\n",
    "#         if all(v >= 3 for v in found.values()): break \n",
    "#         try:\n",
    "#             df = pd.read_parquet(p)\n",
    "#             vid_id = Path(p).stem\n",
    "#             for act in TARGETS:\n",
    "#                 if found[act] >= 3: continue \n",
    "#                 rows = df[df['action'] == act]\n",
    "#                 if not rows.empty:\n",
    "#                     r = rows.iloc[0]\n",
    "#                     events.append({\n",
    "#                         'video_id': vid_id, 'action': act,\n",
    "#                         'frame': (r['start_frame'] + r['stop_frame']) // 2,\n",
    "#                         'ag': r['agent_id'], 'tg': r['target_id']\n",
    "#                     })\n",
    "#                     found[act] += 1\n",
    "#         except: continue\n",
    "#     print(f\"Stats: {found}\")\n",
    "#     return events\n",
    "\n",
    "# # --- RUNNER ---\n",
    "# def run_comparison():\n",
    "#     print(f\"--- FORENSIC VALIDATION (SCALE {SCALE}) ---\")\n",
    "#     EVENTS = get_diverse_events()\n",
    "    \n",
    "#     DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     try: model = EthoSwarmNet(37, 128).to(DEVICE)\n",
    "#     except: print(\"Model class not found.\"); return\n",
    "\n",
    "#     if os.path.exists(MODEL_PATH):\n",
    "#         model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE), strict=False)\n",
    "#     else: print(\" Weights NOT found.\"); return\n",
    "#     model.eval()\n",
    "    \n",
    "#     print(f\"\\n{'VIDEO':<12} | {'ACTION':<10} | {'PREDICTION (Top 3)':<50} | {'INPUT STATS (Speed/Dist)'}\")\n",
    "#     print(\"-\" * 110)\n",
    "    \n",
    "#     for evt in EVENTS:\n",
    "#         ds = ValidationDataset(evt['video_id'], LAB_ID, TRACKING_PATH, evt['ag'], evt['tg'])\n",
    "#         item = ds[0]\n",
    "#         if item is None: continue\n",
    "#         fa_raw, ft_raw, lid = item\n",
    "        \n",
    "#         # Apply Scale 1.0 (Raw)\n",
    "#         fa = (fa_raw / SCALE).to(DEVICE)\n",
    "#         ft = (ft_raw / SCALE).to(DEVICE)\n",
    "#         fa = torch.clamp(fa, -5.0, 5.0)\n",
    "#         ft = torch.clamp(ft, -5.0, 5.0)\n",
    "        \n",
    "#         mid = evt['frame']\n",
    "#         start = max(0, mid - 50); end = min(fa.shape[0], mid + 50)\n",
    "#         if start >= end: continue\n",
    "        \n",
    "#         l_bat = torch.tensor([lid], device=DEVICE)\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             ba = fa[start:end].unsqueeze(0)\n",
    "#             bt = ft[start:end].unsqueeze(0)\n",
    "#             lbl = l_bat.repeat(ba.shape[0])\n",
    "            \n",
    "#             out = model(ba, bt, ba, bt, lbl)\n",
    "#             mid_idx = min(50, out.shape[1]-1)\n",
    "#             probs = out[0, mid_idx].float().cpu().numpy()\n",
    "            \n",
    "#             # --- DIAGNOSTICS ---\n",
    "#             # Extract Speed (Ch 8) and Dist (Ch 10) at midpoint\n",
    "#             # Shape is [1, T, 11, 16], we want average over nodes\n",
    "#             curr_spd = ba[0, mid_idx, :, 8].mean().item()\n",
    "#             curr_dist = ba[0, mid_idx, :, 10].mean().item()\n",
    "#             stats = f\"Spd:{curr_spd:.2f} Dist:{curr_dist:.2f}\"\n",
    "            \n",
    "#             # Top 3\n",
    "#             top3_idx = np.argsort(probs)[-3:][::-1]\n",
    "#             top3 = [f\"{ACTION_LIST[i]}({probs[i]:.2f})\" for i in top3_idx]\n",
    "            \n",
    "#             mark = \"\" if evt['action'] in [ACTION_LIST[i] for i in top3_idx] else \"\"\n",
    "            \n",
    "#             print(f\"{evt['video_id'][:10]:<12} | {evt['action']:<10} | {mark} {', '.join(top3):<47} | {stats}\")\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     run_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3be83b0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T11:34:22.589542Z",
     "iopub.status.busy": "2025-12-09T11:34:22.589350Z",
     "iopub.status.idle": "2025-12-09T11:34:43.977324Z",
     "shell.execute_reply": "2025-12-09T11:34:43.976431Z"
    },
    "papermill": {
     "duration": 21.39697,
     "end_time": "2025-12-09T11:34:43.978540",
     "exception": false,
     "start_time": "2025-12-09T11:34:22.581570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STARTING MODULE 8: FINAL INFERENCE (WINNER-TAKES-ALL) ---\n",
      "Scanning 1 videos for tasks...\n",
      "Total Unique Tasks: 16\n",
      "Weights loaded from /kaggle/input/mabe-separated/ethoswarm_v3_ep3.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc26ba79b4fc4784a2f183fcdc231e43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DEBUG SNAPSHOT (438887472) ---\n",
      "Task: mouse1 -> mouse2\n",
      "Frame 9211 Winner: approach (0.073)\n",
      "Success! Generated 506 rows.\n",
      "Submission saved to submission.csv\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# MODULE 8: FINAL INFERENCE (WINNER-TAKES-ALL)\n",
    "# ==============================================================================\n",
    "import os\n",
    "import gc\n",
    "import glob\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.ndimage import median_filter\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "try:\n",
    "    DEVICE\n",
    "except NameError:\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "TEST_CSV = '/kaggle/input/MABe-mouse-behavior-detection/test.csv'\n",
    "TEST_TRACKING = '/kaggle/input/MABe-mouse-behavior-detection/test_tracking'\n",
    "SUBMISSION_PATH = 'submission.csv'\n",
    "\n",
    "def find_weights(filename='ethoswarm_v3_ep3.pth'):\n",
    "    files = glob.glob(f\"/kaggle/input/**/{filename}\", recursive=True)\n",
    "    if files: return files[0]\n",
    "    return filename \n",
    "\n",
    "WEIGHTS_PATH = find_weights('ethoswarm_v3_ep3.pth')\n",
    "INFERENCE_CHUNK_SIZE = 4000\n",
    "GPU_BATCH_SIZE = 8\n",
    "NUM_WORKERS = 0 \n",
    "\n",
    "# --- 2. THRESHOLDS (Slightly Higher to Reduce Noise) ---\n",
    "TH = {\n",
    "    \"sniff\": 0.08,        \"approach\": 0.08,\n",
    "    \"rear\": 0.20,         \"chase\": 0.10,\n",
    "    \"attack\": 0.20,       \"mount\": 0.20,\n",
    "    \"escape\": 0.10,       \"avoid\": 0.10,\n",
    "    \"climb\": 0.15,        \"biteobject\": 0.15,\n",
    "    \"sniffface\": 0.08,    \"sniffgenital\": 0.08\n",
    "}\n",
    "DEF_TH = 0.05\n",
    "\n",
    "BODY_PARTS = [\"ear_left\", \"ear_right\", \"nose\", \"neck\", \"body_center\",\"lateral_left\", \"lateral_right\", \"hip_left\", \"hip_right\",\"tail_base\", \"tail_tip\"]\n",
    "\n",
    "ACTION_LIST = sorted([\n",
    "    \"allogroom\", \"approach\", \"attack\", \"attemptmount\", \"avoid\", \"biteobject\",\n",
    "    \"chase\", \"chaseattack\", \"climb\", \"defend\", \"dig\", \"disengage\", \"dominance\",\n",
    "    \"dominancegroom\", \"dominancemount\", \"ejaculate\", \"escape\", \"exploreobject\",\n",
    "    \"flinch\", \"follow\", \"freeze\", \"genitalgroom\", \"huddle\", \"intromit\", \"mount\",\n",
    "    \"rear\", \"reciprocalsniff\", \"rest\", \"run\", \"selfgroom\", \"shepherd\", \"sniff\",\n",
    "    \"sniffbody\", \"sniffface\", \"sniffgenital\", \"submit\", \"tussle\"\n",
    "])\n",
    "\n",
    "# Separation for Masking\n",
    "SELF_BEHAVIORS = sorted([\"biteobject\", \"climb\", \"dig\", \"exploreobject\", \"freeze\", \"genitalgroom\", \"huddle\", \"rear\", \"rest\", \"run\", \"selfgroom\"])\n",
    "PAIR_BEHAVIORS = sorted(list(set(ACTION_LIST) - set(SELF_BEHAVIORS)))\n",
    "SELF_IDXS = [ACTION_LIST.index(a) for a in SELF_BEHAVIORS]\n",
    "PAIR_IDXS = [ACTION_LIST.index(a) for a in PAIR_BEHAVIORS]\n",
    "\n",
    "LAB_NAME_TO_IDX = {name: i for i, name in enumerate(sorted(LAB_CONFIGS.keys()))}\n",
    "\n",
    "# ==============================================================================\n",
    "# 8.1 PARSER\n",
    "# ==============================================================================\n",
    "def parse_test_tasks(test_csv_path):\n",
    "    if not os.path.exists(test_csv_path): return []\n",
    "    df = pd.read_csv(test_csv_path)\n",
    "    tasks = []\n",
    "    \n",
    "    print(f\"Scanning {len(df)} videos for tasks...\")\n",
    "    for i, row in df.iterrows():\n",
    "        vid = str(row['video_id'])\n",
    "        lab = row['lab_id']\n",
    "        raw = row.get('behaviors_labeled', None)\n",
    "        try:\n",
    "            if pd.isna(raw):\n",
    "                tasks.append({'vid': vid, 'lab': lab, 'ag': 'mouse1', 'tg': 'mouse2'})\n",
    "                tasks.append({'vid': vid, 'lab': lab, 'ag': 'mouse2', 'tg': 'mouse1'})\n",
    "                continue\n",
    "            \n",
    "            labels = ast.literal_eval(raw)\n",
    "            seen = set()\n",
    "            for item in labels:\n",
    "                if isinstance(item, str): parts = item.split(',')\n",
    "                else: parts = item \n",
    "                \n",
    "                if len(parts) >= 2:\n",
    "                    ag = parts[0].strip()\n",
    "                    tg = parts[1].strip()\n",
    "                    if tg.lower() == 'self' or tg == '': tg = 'self'\n",
    "                    \n",
    "                    if (ag, tg) not in seen:\n",
    "                        tasks.append({'vid': vid, 'lab': lab, 'ag': ag, 'tg': tg})\n",
    "                        seen.add((ag, tg))\n",
    "        except:\n",
    "            tasks.append({'vid': vid, 'lab': lab, 'ag': 'mouse1', 'tg': 'mouse2'})\n",
    "            \n",
    "    print(f\"Total Unique Tasks: {len(tasks)}\")\n",
    "    return tasks\n",
    "\n",
    "# ==============================================================================\n",
    "# 8.2 DATASET\n",
    "# ==============================================================================\n",
    "class TaskInferenceDataset(Dataset):\n",
    "    def __init__(self, tasks, tracking_dir):\n",
    "        self.samples = tasks\n",
    "        self.tracking_dir = Path(tracking_dir)\n",
    "        self.bp_idx = {b:i for i,b in enumerate(BODY_PARTS)}\n",
    "        self.ALIASES = {'hip_left': ['lateral_left', 'tail_base'], 'hip_right': ['lateral_right', 'tail_base'], 'neck': ['headpiece_bottombackright', 'nose'], 'body_center': ['tail_base']}\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "    def _fix_teleport(self, pos):\n",
    "        T, N, _ = pos.shape\n",
    "        missing = (np.abs(pos).sum(axis=2) < 1e-6)\n",
    "        cleaned = pos.copy()\n",
    "        cols = np.where(missing.any(axis=0))[0]\n",
    "        if len(cols) > 0:\n",
    "            t = np.arange(T)\n",
    "            for n in cols:\n",
    "                mask = missing[:, n]\n",
    "                if np.any(~mask):\n",
    "                    cleaned[mask, n, 0] = np.interp(t[mask], t[~mask], pos[~mask, n, 0])\n",
    "                    cleaned[mask, n, 1] = np.interp(t[mask], t[~mask], pos[~mask, n, 1])\n",
    "        return cleaned\n",
    "\n",
    "    def _geo_features(self, m1, m2, lab):\n",
    "        # NORMALIZED SCALE\n",
    "        conf = LAB_CONFIGS.get(lab, LAB_CONFIGS['DEFAULT'])\n",
    "        px = float(conf['pix_cm']) + 1e-6\n",
    "        m1 = m1.astype(np.float32) / px\n",
    "        m2 = m2.astype(np.float32) / px\n",
    "        \n",
    "        # Align (NO ROTATION to match Training)\n",
    "        origin = m1[:, 9:10] # Tail base\n",
    "        centered = m1 - origin\n",
    "        other_centered = m2 - origin\n",
    "        \n",
    "        # Velocity\n",
    "        vel = np.diff(centered, axis=0, prepend=centered[0:1])\n",
    "        speed = np.sqrt((vel**2).sum(axis=-1))\n",
    "        \n",
    "        # Relation\n",
    "        dist = np.sqrt(((m1 - m2)**2).sum(axis=-1))\n",
    "        \n",
    "        # Pack to 16 matching Training\n",
    "        # [Pos X, Pos Y, Vel X, Vel Y, Speed, Rel_Dist] + Pads\n",
    "        feats = np.stack([\n",
    "            centered[...,0], centered[...,1],\n",
    "            vel[...,0], vel[...,1],\n",
    "            speed, dist\n",
    "        ], axis=-1)\n",
    "        \n",
    "        L, N, _ = m1.shape\n",
    "        padded = np.zeros((L, N, 16), dtype=np.float32)\n",
    "        padded[..., :6] = feats\n",
    "        \n",
    "        padded = np.nan_to_num(padded, nan=0.0)\n",
    "        padded = np.clip(padded, -30.0, 30.0) \n",
    "        \n",
    "        return padded\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        job = self.samples[idx]\n",
    "        job_out = job.copy()\n",
    "        \n",
    "        try:\n",
    "            p = self.tracking_dir / job['lab'] / f\"{job['vid']}.parquet\"\n",
    "            if not p.exists(): return torch.zeros(1), 0, job_out\n",
    "\n",
    "            df = pd.read_parquet(p)\n",
    "            max_f = int(df.video_frame.max() + 1) if 'video_frame' in df else len(df)\n",
    "            limit = min(max_f, 3000000)\n",
    "            \n",
    "            uids = df.mouse_id.unique()\n",
    "            def resolve(req):\n",
    "                if req == 'self': return None\n",
    "                for u in uids:\n",
    "                    if str(u) == str(req): return u\n",
    "                    if f\"mouse{u}\" == str(req): return u\n",
    "                return uids[0]\n",
    "\n",
    "            real_ag = resolve(job['ag'])\n",
    "            real_tg = resolve(job['tg'])\n",
    "            \n",
    "            if real_tg is None:\n",
    "                real_tg = real_ag\n",
    "                job_out['is_self'] = True\n",
    "            else:\n",
    "                job_out['is_self'] = False\n",
    "                \n",
    "            job_out['sub_ag'] = f\"mouse{real_ag}\" if str(real_ag).isdigit() else str(real_ag)\n",
    "            job_out['sub_tg'] = 'self' if job_out['is_self'] else (f\"mouse{real_tg}\" if str(real_tg).isdigit() else str(real_tg))\n",
    "\n",
    "            raw_ag = np.zeros((limit, 11, 2), dtype=np.float32)\n",
    "            raw_tg = np.zeros_like(raw_ag)\n",
    "            avail = set(df['bodypart'].unique())\n",
    "            \n",
    "            def extract(mid, dest):\n",
    "                m_rows = df[df.mouse_id == mid]\n",
    "                groups = dict(tuple(m_rows.groupby('bodypart')))\n",
    "                for bp, idx in self.bp_idx.items():\n",
    "                    src = bp\n",
    "                    if src not in avail and bp in self.ALIASES:\n",
    "                        for a in self.ALIASES[bp]:\n",
    "                            if a in avail: src = a; break\n",
    "                    if src in groups:\n",
    "                        g = groups[src]\n",
    "                        f = g.video_frame.values.astype(int) if 'video_frame' in g else np.arange(len(g))\n",
    "                        v = f < limit\n",
    "                        dest[f[v], idx] = g[['x','y']].values[v]\n",
    "\n",
    "            extract(real_ag, raw_ag)\n",
    "            extract(real_tg, raw_tg)\n",
    "\n",
    "            raw_ag = self._fix_teleport(raw_ag)\n",
    "            raw_tg = self._fix_teleport(raw_tg)\n",
    "            for m in [raw_ag, raw_tg]:\n",
    "                tail_base = m[:, 9]\n",
    "                for i in [3, 4, 7, 8]: m[:, i] = tail_base if m[:, i].sum() == 0 else m[:, i]\n",
    "\n",
    "            feats = self._geo_features(raw_ag, raw_tg, job['lab'])\n",
    "            lid = LAB_NAME_TO_IDX.get(job['lab'], 0)\n",
    "            return torch.tensor(feats), lid, job_out\n",
    "\n",
    "        except: return torch.zeros(1), 0, job_out\n",
    "\n",
    "def collate_fn(b): return b[0]\n",
    "\n",
    "# ==============================================================================\n",
    "# 8.3 MAIN EXECUTION\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"--- STARTING MODULE 8: FINAL INFERENCE (WINNER-TAKES-ALL) ---\")\n",
    "    \n",
    "    if not os.path.exists(TEST_CSV):\n",
    "        pd.DataFrame(columns=['row_id','video_id','action']).to_csv(SUBMISSION_PATH, index=False); exit()\n",
    "\n",
    "    tasks = parse_test_tasks(TEST_CSV)\n",
    "    ds = TaskInferenceDataset(tasks, TEST_TRACKING)\n",
    "    loader = DataLoader(ds, batch_size=1, collate_fn=collate_fn, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    try: model = EthoSwarmNet(37, 128).to(DEVICE)\n",
    "    except: print(\"ERROR: EthoSwarmNet not found.\"); exit()\n",
    "\n",
    "    if os.path.exists(WEIGHTS_PATH):\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(WEIGHTS_PATH, map_location=DEVICE), strict=False)\n",
    "            print(f\"Weights loaded from {WEIGHTS_PATH}\")\n",
    "        except: pass\n",
    "    model.eval()\n",
    "    \n",
    "    first_pass = True\n",
    "    with open(SUBMISSION_PATH, 'w') as f:\n",
    "        f.write(\"row_id,video_id,agent_id,target_id,action,start_frame,stop_frame\\n\")\n",
    "        row_id = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for feat, lid, job in tqdm(loader, desc=\"Inferring\"):\n",
    "                if feat.dim() < 3 or feat.shape[0] < 5: continue\n",
    "                \n",
    "                T = feat.shape[0]\n",
    "                rem = T % INFERENCE_CHUNK_SIZE\n",
    "                if rem > 0:\n",
    "                    pad = INFERENCE_CHUNK_SIZE - rem\n",
    "                    feat = torch.cat([feat, feat[-1:].repeat(pad,1,1)], 0)\n",
    "                \n",
    "                c_f = feat.view(-1, INFERENCE_CHUNK_SIZE, 11, 16)\n",
    "                preds_list = []\n",
    "                for i in range(len(c_f)):\n",
    "                    ba = c_f[i:i+1].to(DEVICE)\n",
    "                    l_idx = torch.tensor([lid], device=DEVICE)\n",
    "                    out = model(ba, ba, ba, ba, l_idx)\n",
    "                    preds_list.append(out.cpu())\n",
    "                \n",
    "                # Raw Probs\n",
    "                probs = torch.cat(preds_list, 1).flatten(0, 1)[:T].numpy()\n",
    "                \n",
    "                # 1. Strict Masking\n",
    "                if job['is_self']:\n",
    "                    probs[:, PAIR_IDXS] = 0.0 \n",
    "                else:\n",
    "                    probs[:, SELF_IDXS] = 0.0\n",
    "                \n",
    "                # 2. TOP-1 GATING (The Fix for Concurrency)\n",
    "                # Keep only the single highest probability per frame\n",
    "                topk_vals = np.sort(probs, axis=1)[:, -1:] \n",
    "                mask_k = (probs >= topk_vals).astype(np.float32)\n",
    "                probs *= mask_k\n",
    "                \n",
    "                # 3. Debug\n",
    "                if first_pass:\n",
    "                    print(f\"\\n--- DEBUG SNAPSHOT ({job['vid']}) ---\")\n",
    "                    print(f\"Task: {job['sub_ag']} -> {job['sub_tg']}\")\n",
    "                    mid = T // 2\n",
    "                    top = np.argmax(probs[mid])\n",
    "                    print(f\"Frame {mid} Winner: {ACTION_LIST[top]} ({probs[mid, top]:.3f})\")\n",
    "                    first_pass = False\n",
    "\n",
    "                aid, tid = job['sub_ag'], job['sub_tg']\n",
    "                \n",
    "                for cls_idx, act in enumerate(ACTION_LIST):\n",
    "                    trace = probs[:, cls_idx]\n",
    "                    if trace.max() < 0.01: continue\n",
    "                    \n",
    "                    thresh = TH.get(act, DEF_TH)\n",
    "                    \n",
    "                    smoothed = median_filter(trace, size=7)\n",
    "                    binary = (smoothed > thresh).astype(np.int8)\n",
    "                    \n",
    "                    if binary.sum() == 0: continue\n",
    "                    diffs = np.diff(np.concatenate(([0], binary, [0])))\n",
    "                    starts = np.where(diffs == 1)[0]\n",
    "                    ends = np.where(diffs == -1)[0]\n",
    "                    \n",
    "                    for s, e in zip(starts, ends):\n",
    "                        if (e - s) < 2: continue\n",
    "                        f.write(f\"{row_id},{job['vid']},{aid},{tid},{act},{s},{e}\\n\")\n",
    "                        row_id += 1\n",
    "                \n",
    "                del feat, c_f, probs, preds_list\n",
    "    \n",
    "    print(f\"Success! Generated {row_id} rows.\")\n",
    "    print(f\"Submission saved to {SUBMISSION_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 13874099,
     "isSourceIdPinned": false,
     "sourceId": 59156,
     "sourceType": "competition"
    },
    {
     "datasetId": 8927698,
     "sourceId": 14013904,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8946032,
     "sourceId": 14054122,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 279806245,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 31.731122,
   "end_time": "2025-12-09T11:34:45.206473",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-09T11:34:13.475351",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "08e69cd00d40423aba3d4cae6b8123ed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1e66bdf01c80495c8e8e9c1f91675545": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "21f00b14db8047f4824ec88bc3a27207": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a6f50bd94fbb41efaeeb183035c24a50": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1e66bdf01c80495c8e8e9c1f91675545",
       "placeholder": "",
       "style": "IPY_MODEL_21f00b14db8047f4824ec88bc3a27207",
       "tabbable": null,
       "tooltip": null,
       "value": "Inferring:100%"
      }
     },
     "c9166a7e30514431b4cf6c65ffbd74c0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cbee8258451345c5a56ad30d6a7e3d63": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fdb174fcc62e49c9bae18fe90058d6e6",
       "placeholder": "",
       "style": "IPY_MODEL_08e69cd00d40423aba3d4cae6b8123ed",
       "tabbable": null,
       "tooltip": null,
       "value": "16/16[00:08&lt;00:00,2.31it/s]"
      }
     },
     "d82361093ccf463dab81841f80e7326d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c9166a7e30514431b4cf6c65ffbd74c0",
       "max": 16.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f34d8890c879456a96fbfb179176f892",
       "tabbable": null,
       "tooltip": null,
       "value": 16.0
      }
     },
     "dc26ba79b4fc4784a2f183fcdc231e43": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a6f50bd94fbb41efaeeb183035c24a50",
        "IPY_MODEL_d82361093ccf463dab81841f80e7326d",
        "IPY_MODEL_cbee8258451345c5a56ad30d6a7e3d63"
       ],
       "layout": "IPY_MODEL_ebf4880cbbe142f589e64dfbcd7bee1c",
       "tabbable": null,
       "tooltip": null
      }
     },
     "ebf4880cbbe142f589e64dfbcd7bee1c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f34d8890c879456a96fbfb179176f892": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "fdb174fcc62e49c9bae18fe90058d6e6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
