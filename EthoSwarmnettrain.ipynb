{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92310ba4",
   "metadata": {
    "id": "CJTxnCW5Oh1W",
    "papermill": {
     "duration": 0.003932,
     "end_time": "2025-12-07T21:36:55.354940",
     "exception": false,
     "start_time": "2025-12-07T21:36:55.351008",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Module 1: The Bio-Physics Data Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e8e9809",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T21:36:55.362576Z",
     "iopub.status.busy": "2025-12-07T21:36:55.362320Z",
     "iopub.status.idle": "2025-12-07T21:37:00.135834Z",
     "shell.execute_reply": "2025-12-07T21:37:00.135249Z"
    },
    "id": "hnCptjwOOh1X",
    "papermill": {
     "duration": 4.77903,
     "end_time": "2025-12-07T21:37:00.137100",
     "exception": false,
     "start_time": "2025-12-07T21:36:55.358070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import ast\n",
    "import random\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "LAB_CONFIGS = {\n",
    "    \"AdaptableSnail\":       {\"thresh\": 718.59, \"window\": 120, \"pix_cm\": 14.5},\n",
    "    \"BoisterousParrot\":     {\"thresh\": 50.93,  \"window\": 292, \"pix_cm\": 5.5},\n",
    "    \"CRIM13\":               {\"thresh\": 207.95, \"window\": 117, \"pix_cm\": 14.5},\n",
    "    \"CalMS21_supplemental\": {\"thresh\": 206.05, \"window\": 196, \"pix_cm\": 18.3},\n",
    "    \"CalMS21_task1\":        {\"thresh\": 154.32, \"window\": 140, \"pix_cm\": 18.3},\n",
    "    \"CalMS21_task2\":        {\"thresh\": 177.51, \"window\": 122, \"pix_cm\": 18.3},\n",
    "    \"CautiousGiraffe\":      {\"thresh\": 119.97, \"window\": 67,  \"pix_cm\": 21.0},\n",
    "    \"DeliriousFly\":         {\"thresh\": 97.31,  \"window\": 172, \"pix_cm\": 16.0},\n",
    "    \"ElegantMink\":          {\"thresh\": 88.58,  \"window\": 391, \"pix_cm\": 18.4},\n",
    "    \"GroovyShrew\":          {\"thresh\": 254.45, \"window\": 115, \"pix_cm\": 11.3},\n",
    "    \"InvincibleJellyfish\":  {\"thresh\": 249.33, \"window\": 158, \"pix_cm\": 32.0},\n",
    "    \"JovialSwallow\":        {\"thresh\": 99.68,  \"window\": 62,  \"pix_cm\": 15.3},\n",
    "    \"LyricalHare\":          {\"thresh\": 198.80, \"window\": 361, \"pix_cm\": 10.9},\n",
    "    \"NiftyGoldfinch\":       {\"thresh\": 303.02, \"window\": 78,  \"pix_cm\": 13.5},\n",
    "    \"PleasantMeerkat\":      {\"thresh\": 150.58, \"window\": 32,  \"pix_cm\": 15.8},\n",
    "    \"ReflectiveManatee\":    {\"thresh\": 117.76, \"window\": 97,  \"pix_cm\": 15.0},\n",
    "    \"SparklingTapir\":       {\"thresh\": 281.60, \"window\": 252, \"pix_cm\": 40.0},\n",
    "    \"TranquilPanther\":      {\"thresh\": 133.98, \"window\": 105, \"pix_cm\": 12.3},\n",
    "    \"UppityFerret\":         {\"thresh\": 228.77, \"window\": 55,  \"pix_cm\": 12.7},\n",
    "    \"DEFAULT\":              {\"thresh\": 150.00, \"window\": 128, \"pix_cm\": 15.0}\n",
    "}\n",
    "\n",
    "ACTION_LIST = sorted([\n",
    "    \"allogroom\", \"approach\", \"attack\", \"attemptmount\", \"avoid\", \"biteobject\",\n",
    "    \"chase\", \"chaseattack\", \"climb\", \"defend\", \"dig\", \"disengage\", \"dominance\",\n",
    "    \"dominancegroom\", \"dominancemount\", \"ejaculate\", \"escape\", \"exploreobject\",\n",
    "    \"flinch\", \"follow\", \"freeze\", \"genitalgroom\", \"huddle\", \"intromit\", \"mount\",\n",
    "    \"rear\", \"reciprocalsniff\", \"rest\", \"run\", \"selfgroom\", \"shepherd\", \"sniff\",\n",
    "    \"sniffbody\", \"sniffface\", \"sniffgenital\", \"submit\", \"tussle\"\n",
    "])\n",
    "ACTION_TO_IDX = {a: i for i, a in enumerate(ACTION_LIST)}\n",
    "NUM_CLASSES = len(ACTION_LIST)\n",
    "BODY_PARTS = [\n",
    "    \"ear_left\", \"ear_right\", \"nose\", \"neck\", \"body_center\",\n",
    "    \"lateral_left\", \"lateral_right\", \"hip_left\", \"hip_right\",\n",
    "    \"tail_base\", \"tail_tip\"\n",
    "]\n",
    "PART_TO_IDX = {p: i for i, p in enumerate(BODY_PARTS)}\n",
    "\n",
    "class BioPhysicsDataset(Dataset):\n",
    "    def __init__(self, data_root, mode='train', video_ids=None):\n",
    "        self.root = Path(data_root)\n",
    "        self.mode = mode\n",
    "        # Directory logic\n",
    "        self.tracking_dir = self.root / f\"{mode}_tracking\"\n",
    "        self.annot_dir = self.root / f\"{mode}_annotation\"\n",
    "        \n",
    "        # Load Metadata\n",
    "        self.metadata = pd.read_csv(self.root / f\"{mode}.csv\")\n",
    "        \n",
    "        # Filter Video IDs (e.g., for train/val split)\n",
    "        if video_ids is not None:\n",
    "            self.metadata = self.metadata[self.metadata['video_id'].astype(str).isin(video_ids)]\n",
    "        \n",
    "        # Build samples from metadata DIRECTLY (Skip strict file check to avoid crash)\n",
    "        self.samples = []\n",
    "        for _, row in self.metadata.iterrows():\n",
    "            self.samples.append({\n",
    "                'video_id': str(row['video_id']),\n",
    "                'lab_id': row['lab_id']\n",
    "            })\n",
    "            \n",
    "        # Hardcoded Window\n",
    "        self.local_window = 256\n",
    "        self.max_global_tokens = 2048\n",
    "\n",
    "        # Pre-scan for sampling\n",
    "        self.action_windows = []\n",
    "        if self.mode == 'train':\n",
    "            self._scan_actions_safe()\n",
    "\n",
    "    def _scan_actions_safe(self):\n",
    "        # We try to find files. If not found, we skip optimization, but DO NOT CRASH.\n",
    "        count = 0\n",
    "        print(\"Scanning subset of annotations for sampling...\")\n",
    "        for i, s in enumerate(self.samples):\n",
    "            if i > 500: break # Quick partial scan\n",
    "            p = self.annot_dir / s['lab_id'] / f\"{s['video_id']}.parquet\"\n",
    "            if p.exists():\n",
    "                try:\n",
    "                    df = pd.read_parquet(p)\n",
    "                    # Find centers\n",
    "                    df = df[df['action'].isin(ACTION_TO_IDX)]\n",
    "                    if not df.empty:\n",
    "                        for c in ((df['start_frame'] + df['stop_frame']) // 2).values:\n",
    "                            self.action_windows.append((i, int(c)))\n",
    "                            count += 1\n",
    "                except: pass\n",
    "        if count == 0:\n",
    "            print(\"Warning: No actions scanned. Falling back to random sampling.\")\n",
    "\n",
    "    def _fix_teleport(self, pos):\n",
    "        # pos: [T, 11, 2]\n",
    "        T, N, _ = pos.shape\n",
    "        # Identify holes\n",
    "        missing = (np.abs(pos).sum(axis=2) < 1e-6)\n",
    "        cleaned = pos.copy()\n",
    "        for n in range(N):\n",
    "            m = missing[:, n]\n",
    "            if np.any(m) and not np.all(m):\n",
    "                valid_t = np.where(~m)[0]\n",
    "                missing_t = np.where(m)[0]\n",
    "                cleaned[missing_t, n, 0] = np.interp(missing_t, valid_t, pos[valid_t, n, 0])\n",
    "                cleaned[missing_t, n, 1] = np.interp(missing_t, valid_t, pos[valid_t, n, 1])\n",
    "        return cleaned\n",
    "\n",
    "    def _geo_feats(self, pos, other, pix_cm):\n",
    "        # Simple geometric extractor\n",
    "        # Normalize\n",
    "        pos = pos / pix_cm\n",
    "        other = other / pix_cm\n",
    "        \n",
    "        # Align\n",
    "        origin = pos[:, 9:10, :] # Tail base\n",
    "        centered = pos - origin\n",
    "        other_centered = other - origin\n",
    "        \n",
    "        # Velocity\n",
    "        vel = np.diff(centered, axis=0, prepend=centered[0:1])\n",
    "        speed = np.sqrt((vel**2).sum(axis=-1))\n",
    "        \n",
    "        # Relation\n",
    "        dist = np.sqrt(((pos - other)**2).sum(axis=-1))\n",
    "        \n",
    "        # Pack to 16\n",
    "        # [Pos X, Pos Y, Vel X, Vel Y, Speed, Rel_Dist] + Pads\n",
    "        feat = np.stack([\n",
    "            centered[...,0], centered[...,1],\n",
    "            vel[...,0], vel[...,1],\n",
    "            speed, dist,\n",
    "            np.zeros_like(speed), np.zeros_like(speed), # 7-8\n",
    "            np.zeros_like(speed), np.zeros_like(speed),\n",
    "            np.zeros_like(speed), np.zeros_like(speed),\n",
    "            np.zeros_like(speed), np.zeros_like(speed),\n",
    "            np.zeros_like(speed), np.zeros_like(speed),\n",
    "        ], axis=-1)\n",
    "        \n",
    "        return feat.astype(np.float32)\n",
    "\n",
    "    def _load(self, idx, center=None):\n",
    "        sample = self.samples[idx]\n",
    "        lab = sample['lab_id']\n",
    "        conf = LAB_CONFIGS.get(lab, LAB_CONFIGS['DEFAULT'])\n",
    "        \n",
    "        # Try Loading Track\n",
    "        raw_m1, raw_m2 = np.zeros((1,11,2)), np.zeros((1,11,2))\n",
    "        \n",
    "        fpath = self.tracking_dir / lab / f\"{sample['video_id']}.parquet\"\n",
    "        \n",
    "        # Load Success?\n",
    "        success = False\n",
    "        if fpath.exists():\n",
    "            try:\n",
    "                df = pd.read_parquet(fpath)\n",
    "                mids = df['mouse_id'].unique()\n",
    "                L = len(df)\n",
    "                \n",
    "                # Expand buffer\n",
    "                raw_m1 = np.zeros((L, 11, 2), dtype=np.float32)\n",
    "                raw_m2 = np.zeros((L, 11, 2), dtype=np.float32)\n",
    "                \n",
    "                m1_id = mids[0]\n",
    "                m2_id = mids[1] if len(mids) > 1 else m1_id\n",
    "                \n",
    "                # Check Bodypart column\n",
    "                if 'bodypart' in df.columns:\n",
    "                    for i, bp in enumerate(BODY_PARTS):\n",
    "                        d1 = df[(df['mouse_id']==m1_id) & (df['bodypart']==bp)][['x','y']].values\n",
    "                        if len(d1)>0: raw_m1[:len(d1), i] = d1\n",
    "                        \n",
    "                        d2 = df[(df['mouse_id']==m2_id) & (df['bodypart']==bp)][['x','y']].values\n",
    "                        if len(d2)>0: raw_m2[:len(d2), i] = d2\n",
    "                else:\n",
    "                    # Wide format check\n",
    "                    for col in df.columns:\n",
    "                        if \"mouse1\" in col:\n",
    "                            # simplified parsing\n",
    "                            pass \n",
    "                success = True\n",
    "            except: pass\n",
    "        \n",
    "        if not success:\n",
    "            # DUMMY DATA TO PREVENT CRASH\n",
    "            # Returns a single frame of zeros\n",
    "            L = self.local_window\n",
    "            raw_m1 = np.zeros((L, 11, 2), dtype=np.float32)\n",
    "            raw_m2 = np.zeros((L, 11, 2), dtype=np.float32)\n",
    "\n",
    "        # 1. Teleport Fix\n",
    "        raw_m1 = self._fix_teleport(raw_m1)\n",
    "        raw_m2 = self._fix_teleport(raw_m2)\n",
    "        \n",
    "        # 2. Window\n",
    "        seq_len = len(raw_m1)\n",
    "        if center is None: center = random.randint(0, seq_len)\n",
    "        s = max(0, min(center - self.local_window//2, seq_len - self.local_window))\n",
    "        e = min(s + self.local_window, seq_len)\n",
    "        \n",
    "        idx_slice = np.arange(s, e)\n",
    "        \n",
    "        # 3. Features\n",
    "        feats = self._geo_feats(raw_m1[idx_slice], raw_m2[idx_slice], conf['pix_cm'])\n",
    "        \n",
    "        # 4. Targets\n",
    "        target = torch.zeros((self.local_window, NUM_CLASSES), dtype=torch.float32)\n",
    "        weights = torch.zeros(self.local_window, dtype=torch.float32)\n",
    "        \n",
    "        # Pad\n",
    "        if len(feats) < self.local_window:\n",
    "            pad_n = self.local_window - len(feats)\n",
    "            pad_f = np.zeros((pad_n, 11, 16), dtype=np.float32)\n",
    "            feats = np.concatenate([feats, pad_f], axis=0)\n",
    "            # Weights stay 0 at end\n",
    "            weights[:len(idx_slice)] = 1.0\n",
    "        else:\n",
    "            weights[:] = 1.0\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            ap = self.annot_dir / lab / f\"{sample['video_id']}.parquet\"\n",
    "            if ap.exists():\n",
    "                try:\n",
    "                    adf = pd.read_parquet(ap)\n",
    "                    for _, row in adf.iterrows():\n",
    "                        if row['action'] in ACTION_TO_IDX:\n",
    "                            st, et = int(row['start_frame'])-s, int(row['stop_frame'])-s\n",
    "                            st, et = max(0, st), min(self.local_window, et)\n",
    "                            if st < et: target[st:et, ACTION_TO_IDX[row['action']]] = 1.0\n",
    "                except: pass\n",
    "        \n",
    "        lab_idx = list(LAB_CONFIGS.keys()).index(lab) if lab in LAB_CONFIGS else 0\n",
    "        return torch.tensor(feats), torch.tensor(feats), target, weights, lab_idx\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode=='train' and random.random() < 0.9 and len(self.action_windows)>0:\n",
    "            i, c = self.action_windows[random.randint(0, len(self.action_windows)-1)]\n",
    "            return self._load(i, c)\n",
    "        return self._load(idx)\n",
    "    \n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "def pad_collate_dual(batch):\n",
    "    gx, lx, t, w, lid = zip(*batch)\n",
    "    return torch.stack(gx), torch.stack(lx), torch.stack(t), torch.stack(w), torch.tensor(lid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb2daf0",
   "metadata": {
    "id": "15H3yhZEOh1Y",
    "papermill": {
     "duration": 0.003002,
     "end_time": "2025-12-07T21:37:00.143300",
     "exception": false,
     "start_time": "2025-12-07T21:37:00.140298",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Module 2: The Morphological & Interaction Core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c743f1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T21:37:00.150495Z",
     "iopub.status.busy": "2025-12-07T21:37:00.150162Z",
     "iopub.status.idle": "2025-12-07T21:37:00.163605Z",
     "shell.execute_reply": "2025-12-07T21:37:00.163010Z"
    },
    "id": "xsGFH_FLOh1Z",
    "papermill": {
     "duration": 0.018519,
     "end_time": "2025-12-07T21:37:00.164656",
     "exception": false,
     "start_time": "2025-12-07T21:37:00.146137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CANONICAL GRAPH ADAPTER (Signal Refinement)\n",
    "# ==============================================================================\n",
    "class CanonicalGraphAdapter(nn.Module):\n",
    "    # INPUT: [B, T, 11, 16] (Geometric Features)\n",
    "    def __init__(self, input_nodes=11, canonical_nodes=11, feat_dim=16, num_labs=20):\n",
    "        super().__init__()\n",
    "\n",
    "        # Learnable Projection Matrix: (NumLabs, 11, 11)\n",
    "        # Learns to map tracking artifacts to a canonical topology per lab\n",
    "        self.projection = nn.Parameter(torch.eye(input_nodes).unsqueeze(0).repeat(num_labs, 1, 1))\n",
    "        \n",
    "        # Identity initialization with slight noise\n",
    "        self.projection.data += torch.randn_like(self.projection) * 0.01\n",
    "\n",
    "        # Lab-Specific Bias (Correction for systematic sensor offset)\n",
    "        self.bias = nn.Parameter(torch.zeros(num_labs, 1, canonical_nodes, feat_dim))\n",
    "\n",
    "        # Refinement MLP (Cleans physics calculations)\n",
    "        self.refine = nn.Sequential(\n",
    "            nn.Linear(feat_dim, feat_dim * 2),\n",
    "            nn.LayerNorm(feat_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(feat_dim * 2, feat_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, lab_idx):\n",
    "        # x: (Batch, Time, 11, 16)\n",
    "        # lab_idx: (Batch)\n",
    "        b, t, n, f = x.shape\n",
    "\n",
    "        # 1. Fetch Weights\n",
    "        W = self.projection[lab_idx] # (B, 11, 11)\n",
    "        B = self.bias[lab_idx]       # (B, 1, 11, 16)\n",
    "\n",
    "        # 2. Graph Projection (Node Mixing)\n",
    "        # We process all time-steps in parallel by flattening B*T\n",
    "        x_flat = x.view(-1, n, f) # (B*T, 11, 16)\n",
    "        \n",
    "        # Prepare Projection Matrix: Expand to T, then view as (B*T, 11, 11)\n",
    "        W_flat = W.unsqueeze(1).repeat(1, t, 1, 1).view(-1, n, n)\n",
    "\n",
    "        # Apply Graph Projection: nodes^T * W\n",
    "        # (B*T, 16, 11) @ (B*T, 11, 11) -> (B*T, 16, 11)\n",
    "        x_t = x_flat.transpose(1, 2) \n",
    "        out = torch.bmm(x_t, W_flat) \n",
    "\n",
    "        # 3. Reshape Back & Apply Physics Refinement\n",
    "        out = out.transpose(1, 2).view(b, t, n, f)\n",
    "        out = out + B # Apply Bias\n",
    "        out = self.refine(out)\n",
    "\n",
    "        return out # (Batch, Time, 11, 16)\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. SOCIAL INTERACTION BLOCK (Updated for Geo-Features)\n",
    "# ==============================================================================\n",
    "class SocialInteractionBlock(nn.Module):\n",
    "    def __init__(self, node_dim=16, hidden_dim=64):\n",
    "        super().__init__()\n",
    "\n",
    "        # Relational MLP\n",
    "        # Takes the pre-calc geometric relations from Module 1\n",
    "        # [Rel_X, Rel_Y, Rel_Dist] + [Speed_Self, Speed_Other] (Derived)\n",
    "        self.relational_mlp = nn.Sequential(\n",
    "            nn.Linear(5, 32),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "\n",
    "        self.fusion = nn.Linear(node_dim * 2 + 16, hidden_dim)\n",
    "\n",
    "    def forward(self, agent_canon, target_canon):\n",
    "        # Input: [B, T, 11, 16] (Normalized Egocentric Features)\n",
    "        \n",
    "        # New Feature Map (Module 1):\n",
    "        # 0: PosX, 1: PosY (Self)\n",
    "        # 2: VelX, 3: VelY\n",
    "        # 4: Neighbor PosX, 5: Neighbor PosY (Explicit Relation)\n",
    "        # 6: Neighbor Dist\n",
    "        \n",
    "        # We extract Interaction Context from Node 0 (Body/Nose or Main Axis)\n",
    "        # or aggregate across nodes. Here we take the mean interaction \n",
    "        # features across all nodes for stability.\n",
    "        \n",
    "        # 1. Extract Interaction Features (Ch 4, 5, 6)\n",
    "        # Shape: [B, T, 3] (Mean over nodes)\n",
    "        interaction_raw = agent_canon[..., 4:7].mean(dim=2) \n",
    "        \n",
    "        # 2. Extract Dynamic Differences\n",
    "        # Speed is typically computed in loader, but let's take velocity diffs (Ch 2,3)\n",
    "        # Vel Self (Ch 2,3)\n",
    "        vel_self = agent_canon[..., 2:4].mean(dim=2) \n",
    "        # Vel Other (Inferred/Proxy via target tensor)\n",
    "        vel_targ = target_canon[..., 2:4].mean(dim=2)\n",
    "        \n",
    "        speed_diff = torch.norm(vel_self - vel_targ, dim=-1, keepdim=True)\n",
    "        dot_prod = (vel_self * vel_targ).sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Combine: [Ix, Iy, Dist, SpeedDiff, VelDot] -> 5 Dims\n",
    "        rel_feats = torch.cat([interaction_raw, speed_diff, dot_prod], dim=-1)\n",
    "        \n",
    "        # Embed\n",
    "        rel_embed = self.relational_mlp(rel_feats) # [B, T, 16]\n",
    "\n",
    "        return agent_canon, target_canon, rel_embed\n",
    "\n",
    "# ==============================================================================\n",
    "# WRAPPER: MORPHOLOGICAL INTERACTION CORE\n",
    "# ==============================================================================\n",
    "class MorphologicalInteractionCore(nn.Module):\n",
    "    def __init__(self, num_labs=20):\n",
    "        super().__init__()\n",
    "        # Standard input 11 canonical nodes\n",
    "        self.adapter = CanonicalGraphAdapter(input_nodes=11, canonical_nodes=11, num_labs=num_labs)\n",
    "        self.interaction = SocialInteractionBlock()\n",
    "\n",
    "        # Fusion: (11 nodes * 16 features * 2 agents) + 16 relation = 368\n",
    "        self.frame_fusion = nn.Linear(368, 128)\n",
    "\n",
    "    def forward(self, agent_x, target_x, lab_idx):\n",
    "        # 1. Adapt Topology (Refine Physics/Geometry)\n",
    "        a_c = self.adapter(agent_x, lab_idx)\n",
    "        t_c = self.adapter(target_x, lab_idx)\n",
    "\n",
    "        # 2. Compute Social Relations\n",
    "        # This uses the specific relative features baked into Module 1\n",
    "        _, _, rel_embed = self.interaction(a_c, t_c)\n",
    "\n",
    "        # 3. Flatten for Transformer Input\n",
    "        b, t, n, f = a_c.shape\n",
    "        a_flat = a_c.view(b, t, -1)\n",
    "        t_flat = t_c.view(b, t, -1)\n",
    "\n",
    "        # 4. Dense Fusion\n",
    "        # Fuses Self(A) + Self(B) + Relationship\n",
    "        combined = torch.cat([a_flat, t_flat, rel_embed], dim=-1) # [B, T, 368]\n",
    "        out = self.frame_fusion(combined) # [B, T, 128]\n",
    "\n",
    "        # Returns: \n",
    "        # out -> The Fused Token (used for Global Context / Temporal processing)\n",
    "        # a_c, t_c -> The Canonical Skeletons (used for Physics Gating in Mod 5)\n",
    "        return out, a_c, t_c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e125324",
   "metadata": {
    "id": "zmX6cExTOh1Z",
    "papermill": {
     "duration": 0.002876,
     "end_time": "2025-12-07T21:37:00.170466",
     "exception": false,
     "start_time": "2025-12-07T21:37:00.167590",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Module 3: The Split-Stream Interaction Block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbe3b99a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T21:37:00.177402Z",
     "iopub.status.busy": "2025-12-07T21:37:00.177198Z",
     "iopub.status.idle": "2025-12-07T21:37:00.185445Z",
     "shell.execute_reply": "2025-12-07T21:37:00.184905Z"
    },
    "id": "je8yED4gOh1a",
    "papermill": {
     "duration": 0.013175,
     "end_time": "2025-12-07T21:37:00.186481",
     "exception": false,
     "start_time": "2025-12-07T21:37:00.173306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SplitStreamInteractionBlock(nn.Module):\n",
    "    def __init__(self, node_dim=16, hidden_dim=128):\n",
    "        super(SplitStreamInteractionBlock, self).__init__()\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "        # BRANCH A: SELF-BEHAVIOR STREAM (The \"Me\" Branch)\n",
    "        # ----------------------------------------------------------------------\n",
    "        # Focus: Posture, Grooming, Rearing, Running.\n",
    "        # Input: Strictly LIMITED to the first 4 channels of the Agent (Pos X/Y, Vel, Speed).\n",
    "        # We explicitly block Neighbor information (Channels 4+) from this stream\n",
    "        # to prevent \"Soft Leaks\" (the Self branch learning Pair behaviors).\n",
    "        self.self_input_size = 11 * 4 # 11 Nodes * 4 Feats (Pos/Vel)\n",
    "        \n",
    "        self.self_projector = nn.Sequential(\n",
    "            nn.Linear(self.self_input_size, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "        # BRANCH B: PAIR-BEHAVIOR STREAM (The \"Us\" Branch)\n",
    "        # ----------------------------------------------------------------------\n",
    "        # Focus: Interaction, Distance, Chasing, Fight.\n",
    "        # Input: Agent (Full) + Target (Full) + Interaction Token.\n",
    "        \n",
    "        # 1. Relational Engine\n",
    "        # The new Module 1 (Geo Features) pre-calculates distance/rel_pos in Ch 4-6.\n",
    "        # We extract this directly rather than re-calculating on the fly.\n",
    "        self.relational_mlp = nn.Sequential(\n",
    "            nn.Linear(3, 32), # [Rel_X, Rel_Y, Rel_Dist] averaged over nodes\n",
    "            nn.GELU(),\n",
    "            nn.Linear(32, 32)\n",
    "        )\n",
    "\n",
    "        # 2. Fusion Layer\n",
    "        # Agent (176) + Target (176) + Rel (32) + Roles (2)\n",
    "        full_node_dim = 11 * node_dim # 176\n",
    "        pair_input_dim = (full_node_dim * 2) + 32 + 2\n",
    "        \n",
    "        self.pair_projector = nn.Sequential(\n",
    "            nn.Linear(pair_input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        # Role Tokens (Solves \"Multi-Agent Roles\")\n",
    "        # [1, 0] = \"I am Acting\", [0, 1] = \"I am Receiving\"\n",
    "        self.role_embedding = nn.Parameter(torch.tensor([[1.0, 0.0], [0.0, 1.0]]))\n",
    "\n",
    "    def forward(self, agent_c, target_c):\n",
    "        \"\"\"\n",
    "        agent_c:  [Batch, Time, 11, 16] (Canonical Skeleton w/ Geo Features)\n",
    "        target_c: [Batch, Time, 11, 16] \n",
    "        \"\"\"\n",
    "        batch, time, nodes, feat = agent_c.shape\n",
    "\n",
    "        # ----------------------------------------------------------\n",
    "        # 1. PROCESS SELF STREAM (Strict Slicing)\n",
    "        # ----------------------------------------------------------\n",
    "        # Only take Channels 0,1,2,3 (Pos, Vel). \n",
    "        # Channels 4+ contain Neighbor Relative info -> BLOCKED.\n",
    "        agent_proprioception = agent_c[..., 0:4] # [B, T, 11, 4]\n",
    "        agent_flat_self = agent_proprioception.contiguous().view(batch, time, -1)\n",
    "        \n",
    "        self_feat = self.self_projector(agent_flat_self) # [B, T, 128]\n",
    "\n",
    "        # ----------------------------------------------------------\n",
    "        # 2. PROCESS PAIR STREAM (Full Context)\n",
    "        # ----------------------------------------------------------\n",
    "        # Flatten full skeletons\n",
    "        agent_flat_full = agent_c.view(batch, time, -1)\n",
    "        target_flat_full = target_c.view(batch, time, -1)\n",
    "        \n",
    "        # Extract Relational Data baked into Module 1 output\n",
    "        # Channels: 4 (Neighbor X), 5 (Neighbor Y), 6 (Dist)\n",
    "        # We assume mean interaction across nodes represents the body-level interaction\n",
    "        rel_feats = agent_c[..., 4:7].mean(dim=2) # [B, T, 3]\n",
    "        \n",
    "        # Embed Relation\n",
    "        rel_embed = self.relational_mlp(rel_feats) # [B, T, 32]\n",
    "\n",
    "        # Add Role Tokens (Broadcasting Agent Role [1,0])\n",
    "        role_token = self.role_embedding[0].view(1, 1, 2).expand(batch, time, 2)\n",
    "\n",
    "        # Fuse Pair Features\n",
    "        # Concatenate: Agent(Full) + Target(Full) + Relation + Role\n",
    "        pair_input = torch.cat([\n",
    "            agent_flat_full, \n",
    "            target_flat_full, \n",
    "            rel_embed, \n",
    "            role_token\n",
    "        ], dim=-1)\n",
    "        \n",
    "        pair_feat = self.pair_projector(pair_input) # [B, T, 128]\n",
    "\n",
    "        return self_feat, pair_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893a6b5c",
   "metadata": {
    "id": "-TztXXo9Oh1a",
    "papermill": {
     "duration": 0.002697,
     "end_time": "2025-12-07T21:37:00.191979",
     "exception": false,
     "start_time": "2025-12-07T21:37:00.189282",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Module 4: The Local-Global Chronos Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c5eb16e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T21:37:00.198780Z",
     "iopub.status.busy": "2025-12-07T21:37:00.198569Z",
     "iopub.status.idle": "2025-12-07T21:37:00.212105Z",
     "shell.execute_reply": "2025-12-07T21:37:00.211397Z"
    },
    "id": "jhhoYs3JOh1a",
    "papermill": {
     "duration": 0.01853,
     "end_time": "2025-12-07T21:37:00.213269",
     "exception": false,
     "start_time": "2025-12-07T21:37:00.194739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class LocalGlobalChronosEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=128, hidden_dim=128):\n",
    "        super(LocalGlobalChronosEncoder, self).__init__()\n",
    "\n",
    "        # ======================================================================\n",
    "        # 1. GLOBAL CONTEXT STREAM (The \"Narrative\" Memory)\n",
    "        # ======================================================================\n",
    "        # Processes the 1 FPS Global Pair Features.\n",
    "        # Captures long-term states (e.g., \"Dominance established 10 mins ago\").\n",
    "        self.global_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        self.pos_encoder = PositionalEncoding(hidden_dim, max_len=5000)\n",
    "\n",
    "        global_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=4,\n",
    "            dim_feedforward=512,\n",
    "            batch_first=True,\n",
    "            dropout=0.1,\n",
    "            activation=\"gelu\"\n",
    "        )\n",
    "        self.global_transformer = nn.TransformerEncoder(global_layer, num_layers=2)\n",
    "\n",
    "        # ======================================================================\n",
    "        # 2. LOCAL SELF STREAM (The \"Me\" Branch) - DEEP TCN\n",
    "        # ======================================================================\n",
    "        # Updated: Receptive Field ~2.0 seconds (64 frames)\n",
    "        self.self_tcn = nn.Sequential(\n",
    "            # Frame Level (d=1)\n",
    "            nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            \n",
    "            # Short Range (d=2)\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=2, dilation=2),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            \n",
    "            # Medium Range (d=4)\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=4, dilation=4),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU(),\n",
    "\n",
    "            # Long Range (d=8) -> +16 frames context\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=8, dilation=8),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU(),\n",
    "\n",
    "            # Very Long Range (d=16) -> +32 frames context (TOTAL ~64)\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=16, dilation=16),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # Cross-Attention to Global \n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=4, batch_first=True)\n",
    "        self.self_norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        # ======================================================================\n",
    "        # 3. LOCAL PAIR STREAM (The \"Us\" Branch) - DEEP TCN\n",
    "        # ======================================================================\n",
    "        self.pair_tcn = nn.Sequential(\n",
    "            # Frame Level\n",
    "            nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            \n",
    "            # Short\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=2, dilation=2),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            \n",
    "            # Medium\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=4, dilation=4),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            \n",
    "            # Long (Interaction Buildup)\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=8, dilation=8),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            \n",
    "            # Very Long (Sustained Aggression/Chase)\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=16, dilation=16),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # Cross-Attention to Global\n",
    "        self.pair_attn = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=4, batch_first=True)\n",
    "        self.pair_norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, global_feat, local_self, local_pair):\n",
    "        \"\"\"\n",
    "        global_feat: [Batch, T_g, 128] \n",
    "        local_self:  [Batch, T_l, 128] \n",
    "        local_pair:  [Batch, T_l, 128] \n",
    "        \"\"\"\n",
    "\n",
    "        # --- A. Build Global Memory Bank ---\n",
    "        g_emb = self.global_proj(global_feat)\n",
    "        g_emb = self.pos_encoder(g_emb)\n",
    "        global_memory = self.global_transformer(g_emb) # [B, T_g, 128]\n",
    "\n",
    "        # --- B. Process Local Self Stream ---\n",
    "        # 1. TCN \n",
    "        s_in = local_self.permute(0, 2, 1) # [B, C, T]\n",
    "        s_tcn = self.self_tcn(s_in).permute(0, 2, 1) # [B, T, C]\n",
    "\n",
    "        # 2. Cross-Attention\n",
    "        # Query: Local TCN, Key/Value: Global Memory\n",
    "        s_ctx, _ = self.self_attn(query=s_tcn, key=global_memory, value=global_memory)\n",
    "        self_out = self.self_norm(s_tcn + s_ctx) \n",
    "\n",
    "        # --- C. Process Local Pair Stream ---\n",
    "        # 1. TCN\n",
    "        p_in = local_pair.permute(0, 2, 1)\n",
    "        p_tcn = self.pair_tcn(p_in).permute(0, 2, 1)\n",
    "\n",
    "        # 2. Cross-Attention\n",
    "        p_ctx, _ = self.pair_attn(query=p_tcn, key=global_memory, value=global_memory)\n",
    "        pair_out = self.pair_norm(p_tcn + p_ctx)\n",
    "\n",
    "        return self_out, pair_out\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=10000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        L = x.size(1)\n",
    "        if L > self.pe.size(0):\n",
    "            return x + self.pe[:self.pe.size(0), :].repeat(math.ceil(L/self.pe.size(0)), 1)[:L, :]\n",
    "        return x + self.pe[:L, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14090e4",
   "metadata": {
    "id": "gCgdUfDjOh1b",
    "papermill": {
     "duration": 0.00276,
     "end_time": "2025-12-07T21:37:00.218890",
     "exception": false,
     "start_time": "2025-12-07T21:37:00.216130",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Module 5: The Multi-Task Logic Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f7ba6aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T21:37:00.225720Z",
     "iopub.status.busy": "2025-12-07T21:37:00.225529Z",
     "iopub.status.idle": "2025-12-07T21:37:00.233556Z",
     "shell.execute_reply": "2025-12-07T21:37:00.232975Z"
    },
    "id": "DpqbgVe-Oh1b",
    "papermill": {
     "duration": 0.0129,
     "end_time": "2025-12-07T21:37:00.234586",
     "exception": false,
     "start_time": "2025-12-07T21:37:00.221686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiTaskLogicHead(nn.Module):\n",
    "    def __init__(self, input_dim=128, num_labs=20):\n",
    "        super(MultiTaskLogicHead, self).__init__()\n",
    "\n",
    "        # 1. DOMAIN EMBEDDING\n",
    "        self.lab_embedding = nn.Embedding(num_labs, 32)\n",
    "\n",
    "        # 2. FEATURE EXPANSION\n",
    "        fusion_dim = input_dim + 32\n",
    "        expanded_dim = 256 \n",
    "\n",
    "        # 3. HEAD A: SELF BEHAVIORS\n",
    "        self.self_classifier = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, expanded_dim),\n",
    "            nn.LayerNorm(expanded_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(expanded_dim, 11) \n",
    "        )\n",
    "\n",
    "        # 4. HEAD B: PAIR BEHAVIORS\n",
    "        self.pair_classifier = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, expanded_dim),\n",
    "            nn.LayerNorm(expanded_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(expanded_dim, 26) \n",
    "        )\n",
    "\n",
    "        # 5. CENTER REGRESSOR\n",
    "        self.center_regressor = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(64, 1) \n",
    "        )\n",
    "\n",
    "        # 6. PHYSICS LOGIC GATE \n",
    "        self.gate_control = nn.Linear(1, 1)\n",
    "        \n",
    "        # --- CRITICAL FIX: INITIALIZATION ---\n",
    "        with torch.no_grad():\n",
    "            # Force Gate Open (start unbiased)\n",
    "            self.gate_control.bias.fill_(2.0)\n",
    "            \n",
    "            # FORCE CLASSIFIERS TO PREDICT BACKGROUND (Prob ~0.01)\n",
    "            # The final Linear layer is at index [3] of Sequential\n",
    "            # Logits = -4.59 -> Sigmoid(-4.59) = 0.01\n",
    "            nn.init.constant_(self.self_classifier[3].bias, -4.59)\n",
    "            nn.init.constant_(self.pair_classifier[3].bias, -4.59)\n",
    "            \n",
    "            # Start Center Regression at 0.5 (Midpoint)\n",
    "            nn.init.constant_(self.center_regressor[2].bias, 0.0)\n",
    "\n",
    "    def forward(self, self_feat, pair_feat, lab_idx, agent_c, target_c):\n",
    "        \"\"\"\n",
    "        self_probs: [B, T, 11] (0.0 - 1.0)\n",
    "        pair_probs: [B, T, 26] (0.0 - 1.0)\n",
    "        \"\"\"\n",
    "        batch, time, _ = self_feat.shape\n",
    "\n",
    "        # A. Context\n",
    "        lab_context = self.lab_embedding(lab_idx).unsqueeze(1).expand(-1, time, -1)\n",
    "        self_input = torch.cat([self_feat, lab_context], dim=-1)\n",
    "        pair_input = torch.cat([pair_feat, lab_context], dim=-1)\n",
    "\n",
    "        # B. Raw Logits\n",
    "        self_logits = self.self_classifier(self_input) \n",
    "        pair_logits = self.pair_classifier(pair_input) \n",
    "        \n",
    "        # Center Score\n",
    "        center_score = torch.sigmoid(self.center_regressor(pair_input))\n",
    "\n",
    "        # C. Physics Gate\n",
    "        # Dist Logic\n",
    "        a_pos = agent_c[:, :, 0, :2]\n",
    "        t_pos = target_c[:, :, 0, :2]\n",
    "        dist = torch.norm(a_pos - t_pos, dim=-1, keepdim=True) \n",
    "\n",
    "        gate = torch.sigmoid(self.gate_control(dist))\n",
    "\n",
    "        # D. Activation\n",
    "        self_probs = torch.sigmoid(self_logits)\n",
    "        \n",
    "        # Combine Pair Logits with Gate\n",
    "        pair_probs = torch.sigmoid(pair_logits) * gate\n",
    "\n",
    "        return self_probs, pair_probs, center_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3395cd1d",
   "metadata": {
    "id": "zpQcsEIpOh1b",
    "papermill": {
     "duration": 0.00285,
     "end_time": "2025-12-07T21:37:00.240238",
     "exception": false,
     "start_time": "2025-12-07T21:37:00.237388",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Module 6: Final Assembly (EthoSwarmNet V3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "325ea5bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T21:37:00.246790Z",
     "iopub.status.busy": "2025-12-07T21:37:00.246583Z",
     "iopub.status.idle": "2025-12-07T21:37:00.255723Z",
     "shell.execute_reply": "2025-12-07T21:37:00.255175Z"
    },
    "id": "B_ZbjAuwOh1b",
    "papermill": {
     "duration": 0.013742,
     "end_time": "2025-12-07T21:37:00.256670",
     "exception": false,
     "start_time": "2025-12-07T21:37:00.242928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ==============================================================================\n",
    "# BEHAVIOR DEFINITIONS (For Output Stitching)\n",
    "# ==============================================================================\n",
    "# All 37 actions sorted alphabetically (Competition Standard)\n",
    "ACTION_LIST = sorted([\n",
    "    \"allogroom\", \"approach\", \"attack\", \"attemptmount\", \"avoid\", \"biteobject\",\n",
    "    \"chase\", \"chaseattack\", \"climb\", \"defend\", \"dig\", \"disengage\", \"dominance\",\n",
    "    \"dominancegroom\", \"dominancemount\", \"ejaculate\", \"escape\", \"exploreobject\",\n",
    "    \"flinch\", \"follow\", \"freeze\", \"genitalgroom\", \"huddle\", \"intromit\", \"mount\",\n",
    "    \"rear\", \"reciprocalsniff\", \"rest\", \"run\", \"selfgroom\", \"shepherd\", \"sniff\",\n",
    "    \"sniffbody\", \"sniffface\", \"sniffgenital\", \"submit\", \"tussle\"\n",
    "])\n",
    "\n",
    "# Subset: 11 Self Behaviors (Agent only)\n",
    "SELF_BEHAVIORS = sorted([\n",
    "    \"biteobject\", \"climb\", \"dig\", \"exploreobject\", \"freeze\", \"genitalgroom\",\n",
    "    \"huddle\", \"rear\", \"rest\", \"run\", \"selfgroom\"\n",
    "])\n",
    "\n",
    "# Subset: 26 Pair Behaviors (Agent + Target)\n",
    "PAIR_BEHAVIORS = sorted([\n",
    "    \"allogroom\", \"approach\", \"attack\", \"attemptmount\", \"avoid\", \"chase\",\n",
    "    \"chaseattack\", \"defend\", \"disengage\", \"dominance\", \"dominancegroom\",\n",
    "    \"dominancemount\", \"ejaculate\", \"escape\", \"flinch\", \"follow\", \"intromit\",\n",
    "    \"mount\", \"reciprocalsniff\", \"shepherd\", \"sniff\", \"sniffbody\", \"sniffface\",\n",
    "    \"sniffgenital\", \"submit\", \"tussle\"\n",
    "])\n",
    "\n",
    "class EthoSwarmNet(nn.Module):\n",
    "    def __init__(self, num_classes=37, input_dim=128):\n",
    "        super(EthoSwarmNet, self).__init__()\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "        # 1. Morphological Core (Module 2)\n",
    "        # ----------------------------------------------------------------------\n",
    "        self.morph_core = MorphologicalInteractionCore(num_labs=20)\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "        # 2. Split-Stream Block (Module 3)\n",
    "        # ----------------------------------------------------------------------\n",
    "        self.split_interaction = SplitStreamInteractionBlock(hidden_dim=128)\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "        # 3. Local-Global Chronos (Module 4)\n",
    "        # ----------------------------------------------------------------------\n",
    "        self.chronos = LocalGlobalChronosEncoder(input_dim=128, hidden_dim=128)\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "        # 4. Multi-Task Logic Head (Module 5)\n",
    "        # ----------------------------------------------------------------------\n",
    "        self.logic_head = MultiTaskLogicHead(\n",
    "            input_dim=128,\n",
    "            num_labs=20\n",
    "        )\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "        # 5. Output Stitching Maps\n",
    "        # ----------------------------------------------------------------------\n",
    "        self.register_buffer('self_indices', self._get_indices(SELF_BEHAVIORS))\n",
    "        self.register_buffer('pair_indices', self._get_indices(PAIR_BEHAVIORS))\n",
    "\n",
    "    def _get_indices(self, subset_list):\n",
    "        indices = []\n",
    "        for beh in subset_list:\n",
    "            try:\n",
    "                indices.append(ACTION_LIST.index(beh))\n",
    "            except ValueError:\n",
    "                pass\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def forward(self, global_agent, global_target, local_agent, local_target, lab_idx):\n",
    "        \"\"\"\n",
    "        The V3 Forward Pass:\n",
    "        Global/Local Streams -> Topology -> Split -> Time -> Logic -> Stitch\n",
    "        \"\"\"\n",
    "\n",
    "        # --- A. TOPOLOGY (Module 2) ---\n",
    "        g_out, _, _ = self.morph_core(global_agent, global_target, lab_idx)\n",
    "        _, l_ac, l_tc = self.morph_core(local_agent, local_target, lab_idx)\n",
    "\n",
    "        # --- B. SPLIT-STREAM (Module 3) ---\n",
    "        l_self, l_pair = self.split_interaction(l_ac, l_tc)\n",
    "\n",
    "        # --- C. TIME & CONTEXT (Module 4) ---\n",
    "        t_self, t_pair = self.chronos(g_out, l_self, l_pair)\n",
    "\n",
    "        # --- D. LOGIC & PHYSICS (Module 5) ---\n",
    "        # FIX: Now accepts 3 return values\n",
    "        # center_score is the Regression Head output (0.0 to 1.0)\n",
    "        p_self, p_pair, center_score = self.logic_head(t_self, t_pair, lab_idx, l_ac, l_tc)\n",
    "\n",
    "        # --- E. OUTPUT STITCHING ---\n",
    "        batch, time, _ = p_self.shape\n",
    "        # Reconstruct [Batch, T, 37] for classification targets\n",
    "        final_output = torch.zeros(batch, time, 37, device=p_self.device, dtype=p_self.dtype)\n",
    "\n",
    "        final_output.index_copy_(2, self.self_indices, p_self)\n",
    "        final_output.index_copy_(2, self.pair_indices, p_pair)\n",
    "        \n",
    "        # NOTE: For now, we are returning 'final_output' (37 classes) \n",
    "        # because the Training Loop expects [B, T, 37] matching targets.\n",
    "        # The 'center_score' improves internal gradient flow via backprop on Module 5.\n",
    "        # If you want to use Center Score explicitly in loss later, return it as tuple:\n",
    "        # return final_output, center_score\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e5db95",
   "metadata": {
    "id": "NY0jnlvPOh1b",
    "papermill": {
     "duration": 0.002717,
     "end_time": "2025-12-07T21:37:00.262111",
     "exception": false,
     "start_time": "2025-12-07T21:37:00.259394",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Module 7: The Training Loop & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e8b2ee5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T21:37:00.268638Z",
     "iopub.status.busy": "2025-12-07T21:37:00.268426Z",
     "iopub.status.idle": "2025-12-08T01:10:25.730654Z",
     "shell.execute_reply": "2025-12-08T01:10:25.729578Z"
    },
    "id": "nze-CLBmOh1b",
    "outputId": "33d30aca-3c20-4fd9-986a-67f87511c3a1",
    "papermill": {
     "duration": 12805.469962,
     "end_time": "2025-12-08T01:10:25.734821",
     "exception": false,
     "start_time": "2025-12-07T21:37:00.264859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training on 2 GPU(s) | Batch Size: 16\n",
      "Scanning subset of annotations for sampling...\n",
      "Scanning subset of annotations for sampling...\n",
      "--> Activating Distributed Data Parallel on 2 GPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a94b1ae67524e4f9a68bc36a34fafdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ep 1:   0%|          | 0/495 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating...\n",
      "Val Loss: 0.1938\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db683f166444c21b151edbe94ffdbe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ep 2:   0%|          | 0/495 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating...\n",
      "Val Loss: 0.1975\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0206c638d7184d439d9eb2330dbac0b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ep 3:   0%|          | 0/495 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating...\n",
      "Val Loss: 0.1949\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ==============================================================================\n",
    "# UTILS & METRICS\n",
    "# ==============================================================================\n",
    "def load_lab_vocabulary(vocab_path, action_to_idx, num_classes, device):\n",
    "    \"\"\"\n",
    "    Loads a boolean mask [20, 37] where 1.0 means the lab annotates that action.\n",
    "    \"\"\"\n",
    "    # Default to \"Allow All\" if file missing\n",
    "    if not os.path.exists(vocab_path):\n",
    "        return torch.ones(25, 37).to(device)\n",
    "        \n",
    "    with open(vocab_path, 'r') as f:\n",
    "        vocab = json.load(f)\n",
    "    \n",
    "    # Must sort keys to match Module 1 index order\n",
    "    lab_names = sorted(list(LAB_CONFIGS.keys()))\n",
    "    mask = torch.zeros(len(lab_names), num_classes).to(device)\n",
    "    \n",
    "    for i, name in enumerate(lab_names):\n",
    "        if name in vocab:\n",
    "            for a in vocab[name]:\n",
    "                if a in action_to_idx: \n",
    "                    mask[i, action_to_idx[a]] = 1.0\n",
    "        else:\n",
    "            mask[i, :] = 1.0\n",
    "    return mask\n",
    "\n",
    "def get_batch_f1(probs_in, targets, batch_vocab_mask, temporal_weights):\n",
    "    \"\"\"\n",
    "    FIXED: Removed torch.sigmoid(). Input 'probs_in' is already 0.0-1.0 from Model.\n",
    "    \"\"\"\n",
    "    # 1. Binarize Predictions (probs are already 0-1)\n",
    "    preds = (probs_in > 0.4).float() \n",
    "    \n",
    "    # 2. Combine Masks\n",
    "    valid_pixels = temporal_weights.unsqueeze(-1) * batch_vocab_mask.unsqueeze(1)\n",
    "    \n",
    "    # 3. Calculate F1 only on VALID pixels\n",
    "    tp = (preds * targets * valid_pixels).sum()\n",
    "    fp = (preds * (1-targets) * valid_pixels).sum()\n",
    "    fn = ((1-preds) * targets * valid_pixels).sum()\n",
    "    \n",
    "    f1 = 2*tp / (2*tp + fp + fn + 1e-6)\n",
    "    return f1.item()\n",
    "\n",
    "# ==============================================================================\n",
    "# LOSS FUNCTION\n",
    "# ==============================================================================\n",
    "class DualStreamMaskedLoss(nn.Module):\n",
    "    def __init__(self, model_self_indices, model_pair_indices):\n",
    "        super().__init__()\n",
    "        self.self_idx = model_self_indices\n",
    "        self.pair_idx = model_pair_indices\n",
    "\n",
    "    def forward(self, model_output_probs, target, weight_mask, lab_vocab_mask):\n",
    "        \"\"\"\n",
    "        FIXED: Removed torch.sigmoid(). \n",
    "        Model outputs probabilities (0-1) due to Physics Gate.\n",
    "        \"\"\"\n",
    "        # Slice Output/Target\n",
    "        # Inputs are ALREADY PROBABILITIES\n",
    "        p_self = model_output_probs[:, :, self.self_idx]\n",
    "        p_pair = model_output_probs[:, :, self.pair_idx]\n",
    "        \n",
    "        t_self = target[:, :, self.self_idx]\n",
    "        t_pair = target[:, :, self.pair_idx]\n",
    "        \n",
    "        # Clamp for numerical stability (prevent log(0))\n",
    "        p_self = torch.clamp(p_self, 1e-7, 1 - 1e-7)\n",
    "        p_pair = torch.clamp(p_pair, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # Slice Lab Masks for Batch\n",
    "        m_self = lab_vocab_mask[:, self.self_idx].unsqueeze(1) # [B, 1, n_self]\n",
    "        m_pair = lab_vocab_mask[:, self.pair_idx].unsqueeze(1) # [B, 1, n_pair]\n",
    "        \n",
    "        # Temporal Mask [B, T, 1]\n",
    "        tm = weight_mask.unsqueeze(-1)\n",
    "        \n",
    "        # Compute Loss (Standard BCELoss, NOT WithLogits)\n",
    "        l_self_raw = F.binary_cross_entropy(p_self, t_self, reduction='none')\n",
    "        l_pair_raw = F.binary_cross_entropy(p_pair, t_pair, reduction='none')\n",
    "        \n",
    "        # Weighted Sum\n",
    "        loss_s = (l_self_raw * m_self * tm).sum() / ((m_self * tm).sum() + 1e-6)\n",
    "        loss_p = (l_pair_raw * m_pair * tm).sum() / ((m_pair * tm).sum() + 1e-6)\n",
    "        \n",
    "        return loss_s + loss_p\n",
    "\n",
    "# ==============================================================================\n",
    "# TRAINING CONTROLLER\n",
    "# ==============================================================================\n",
    "def train_ethoswarm_v3():\n",
    "    # --- 1. SETUP & PATHS ---\n",
    "    if 'mabe_mouse_behavior_detection_path' in globals():\n",
    "        DATA_PATH = globals()['mabe_mouse_behavior_detection_path']\n",
    "    elif os.path.exists('/kaggle/input/MABe-mouse-behavior-detection'):\n",
    "        DATA_PATH = '/kaggle/input/MABe-mouse-behavior-detection'\n",
    "    else: \n",
    "        print(\"Dataset not found.\"); return\n",
    "    \n",
    "    VOCAB_PATH = '/kaggle/input/mabe-metadata/results/lab_vocabulary.json'\n",
    "    \n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    BATCH_SIZE = 8 * max(1, gpu_count)\n",
    "    LEARNING_RATE = 3e-4 \n",
    "    NUM_EPOCHS = 3 \n",
    "\n",
    "    print(f\"Start Training on {gpu_count} GPU(s) | Batch Size: {BATCH_SIZE}\")\n",
    "\n",
    "    # --- 2. DATA PREP (Strict Video Split) ---\n",
    "    meta = pd.read_csv(f\"{DATA_PATH}/train.csv\")\n",
    "    vids = meta['video_id'].astype(str).unique()\n",
    "    np.random.shuffle(vids)\n",
    "    \n",
    "    split = int(len(vids) * 0.90)\n",
    "    train_ids = vids[:split]\n",
    "    val_ids = vids[split:]\n",
    "    \n",
    "    # Loaders - using Module 1 (Cached)\n",
    "    train_ds = BioPhysicsDataset(DATA_PATH, 'train', video_ids=train_ids)\n",
    "    val_ds = BioPhysicsDataset(DATA_PATH, 'train', video_ids=val_ids)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_collate_dual, num_workers=2)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=pad_collate_dual, num_workers=2)\n",
    "    \n",
    "    # --- 3. MODEL INITIALIZATION ---\n",
    "    model = EthoSwarmNet(num_classes=NUM_CLASSES, input_dim=128)\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    if gpu_count > 1:\n",
    "        print(f\"--> Activating Distributed Data Parallel on {gpu_count} GPUs\")\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=LEARNING_RATE, steps_per_epoch=len(train_loader), epochs=NUM_EPOCHS)\n",
    "    \n",
    "    # Load Masks\n",
    "    lab_masks = load_lab_vocabulary(VOCAB_PATH, ACTION_TO_IDX, NUM_CLASSES, DEVICE)\n",
    "    \n",
    "    self_indices = [ACTION_TO_IDX[a] for a in sorted(\n",
    "        [\"biteobject\", \"climb\", \"dig\", \"exploreobject\", \"freeze\", \"genitalgroom\", \n",
    "         \"huddle\", \"rear\", \"rest\", \"run\", \"selfgroom\"])]\n",
    "    pair_indices = [ACTION_TO_IDX[a] for a in sorted(\n",
    "        [\"allogroom\", \"approach\", \"attack\", \"attemptmount\", \"avoid\", \"chase\", \n",
    "         \"chaseattack\", \"defend\", \"disengage\", \"dominance\", \"dominancegroom\", \n",
    "         \"dominancemount\", \"ejaculate\", \"escape\", \"flinch\", \"follow\", \"intromit\", \n",
    "         \"mount\", \"reciprocalsniff\", \"shepherd\", \"sniff\", \"sniffbody\", \"sniffface\", \n",
    "         \"sniffgenital\", \"submit\", \"tussle\"])]\n",
    "         \n",
    "    loss_fn = DualStreamMaskedLoss(self_indices, pair_indices)\n",
    "\n",
    "    # --- 4. EPOCH LOOP ---\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        loop = tqdm(train_loader, desc=f\"Ep {epoch+1}\")\n",
    "        \n",
    "        run_loss = 0.0\n",
    "        run_f1 = 0.0\n",
    "        \n",
    "        for i, batch in enumerate(loop):\n",
    "            # Move 5 items to GPU\n",
    "            gx, lx, tgt, weights, lid = [b.to(DEVICE) for b in batch]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward \n",
    "            # Output is PROBABILITIES now (from Module 5+6)\n",
    "            probs = model(gx, gx, lx, lx, lid)\n",
    "            \n",
    "            # Loss Calc \n",
    "            loss = loss_fn(probs, tgt, weights, lab_masks[lid])\n",
    "            \n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Metrics\n",
    "            with torch.no_grad():\n",
    "                # FIXED: Don't sigmoid again\n",
    "                f1 = get_batch_f1(probs, tgt, lab_masks[lid], weights)\n",
    "                \n",
    "            run_loss = 0.9*run_loss + 0.1*loss.item() if i>0 else loss.item()\n",
    "            run_f1 = 0.9*run_f1 + 0.1*f1 if i>0 else f1\n",
    "            \n",
    "            if i % 20 == 0:\n",
    "                loop.set_postfix({'Loss': f\"{run_loss:.4f}\", 'F1': f\"{run_f1:.3f}\"})\n",
    "        \n",
    "        # Validation\n",
    "        print(\"Validating...\")\n",
    "        model.eval()\n",
    "        val_loss_sum = 0\n",
    "        val_f1_sum = 0.0\n",
    "        batches = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                gx, lx, tgt, weights, lid = [b.to(DEVICE) for b in batch]\n",
    "                \n",
    "                probs = model(gx, gx, lx, lx, lid)\n",
    "                loss = loss_fn(probs, tgt, weights, lab_masks[lid])\n",
    "                \n",
    "                # Metric\n",
    "                f1 = get_batch_f1(probs, tgt, lab_masks[lid], weights)\n",
    "                \n",
    "                val_loss_sum += loss.item()\n",
    "                val_f1_sum += f1\n",
    "                batches += 1\n",
    "                \n",
    "        print(f\"Val Loss: {val_loss_sum/batches:.4f} | Val F1: {val_f1_sum/batches:.4f}\")\n",
    "        \n",
    "        state = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\n",
    "        torch.save(state, f\"ethoswarm_v3_ep{epoch+1}.pth\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_ethoswarm_v3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af42d3dd",
   "metadata": {
    "id": "S-eQVEZwOh1c",
    "papermill": {
     "duration": 0.003617,
     "end_time": "2025-12-08T01:10:25.741975",
     "exception": false,
     "start_time": "2025-12-08T01:10:25.738358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13874099,
     "isSourceIdPinned": false,
     "sourceId": 59156,
     "sourceType": "competition"
    },
    {
     "datasetId": 8927698,
     "sourceId": 14013904,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 279806245,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12816.552218,
   "end_time": "2025-12-08T01:10:28.564637",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-07T21:36:52.012419",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0206c638d7184d439d9eb2330dbac0b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_563b8599a0da49ea921c6c8fbaec6a35",
        "IPY_MODEL_acb678996bb94ac5bf22ef7005f2fb37",
        "IPY_MODEL_4d8cd1a73ff84e92949a714f6ae6231a"
       ],
       "layout": "IPY_MODEL_83cd08acb20a46aa934ec91489df384d",
       "tabbable": null,
       "tooltip": null
      }
     },
     "0d58e1246de74d859edc268131acf5e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "20beb3ae7cd54eb8bd72e9f06c560186": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "23873050c5524aebb0cda89c41dca3be": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "24055b84801b4db1bf833c10cda8183f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a9b1f1a1f3524c3598c554e81ebc900c",
       "placeholder": "",
       "style": "IPY_MODEL_eec0023b0a324f8a8f51a3e11a9b920e",
       "tabbable": null,
       "tooltip": null,
       "value": "Ep1:100%"
      }
     },
     "2a94b1ae67524e4f9a68bc36a34fafdd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_24055b84801b4db1bf833c10cda8183f",
        "IPY_MODEL_4b7203d6fe1247e68b2184bc2e9bdbf4",
        "IPY_MODEL_99a3e60d57c040daaf5c28429143072e"
       ],
       "layout": "IPY_MODEL_96218318e75d4a148eec4a2a13fd240a",
       "tabbable": null,
       "tooltip": null
      }
     },
     "30de0708351841d3b54733153424bf98": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "310cae6ad7434f00ab400487be0d8489": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b6be36f9d12944af993a042374cac188",
       "max": 495.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_bcf0c3cd05a344c7b77d0a1e2407751c",
       "tabbable": null,
       "tooltip": null,
       "value": 495.0
      }
     },
     "3aee2fdfd6f0448e9239af7981fe1c86": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4b7203d6fe1247e68b2184bc2e9bdbf4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b0a7b09cd99c4892bf54d33544a8cf3c",
       "max": 495.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3aee2fdfd6f0448e9239af7981fe1c86",
       "tabbable": null,
       "tooltip": null,
       "value": 495.0
      }
     },
     "4d8cd1a73ff84e92949a714f6ae6231a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f573f18657c0461f8d1bda36cf6df116",
       "placeholder": "",
       "style": "IPY_MODEL_30de0708351841d3b54733153424bf98",
       "tabbable": null,
       "tooltip": null,
       "value": "495/495[1:04:09&lt;00:00,7.51s/it,Loss=0.1896,F1=0.533]"
      }
     },
     "4db683f166444c21b151edbe94ffdbe1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a96c0446b14d43ea8b204636b4908eee",
        "IPY_MODEL_310cae6ad7434f00ab400487be0d8489",
        "IPY_MODEL_aad1375b2c9549459d477949344ade7c"
       ],
       "layout": "IPY_MODEL_9a9bcdc855fc43838ea2346c5f5282db",
       "tabbable": null,
       "tooltip": null
      }
     },
     "563b8599a0da49ea921c6c8fbaec6a35": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_23873050c5524aebb0cda89c41dca3be",
       "placeholder": "",
       "style": "IPY_MODEL_867f230b66f14c3ebeaf91fe150c57cc",
       "tabbable": null,
       "tooltip": null,
       "value": "Ep3:100%"
      }
     },
     "83cd08acb20a46aa934ec91489df384d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "867f230b66f14c3ebeaf91fe150c57cc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "96218318e75d4a148eec4a2a13fd240a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "99a3e60d57c040daaf5c28429143072e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e866cdcc8f6e4bb2b0d8f6be2da7487a",
       "placeholder": "",
       "style": "IPY_MODEL_20beb3ae7cd54eb8bd72e9f06c560186",
       "tabbable": null,
       "tooltip": null,
       "value": "495/495[1:03:21&lt;00:00,5.17s/it,Loss=0.2444,F1=0.430]"
      }
     },
     "9a9bcdc855fc43838ea2346c5f5282db": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a31461435ea141fda9f31cd630348909": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a96c0446b14d43ea8b204636b4908eee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a31461435ea141fda9f31cd630348909",
       "placeholder": "",
       "style": "IPY_MODEL_0d58e1246de74d859edc268131acf5e0",
       "tabbable": null,
       "tooltip": null,
       "value": "Ep2:100%"
      }
     },
     "a9b1f1a1f3524c3598c554e81ebc900c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aad1375b2c9549459d477949344ade7c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_faa46a67308b49e693a2a11c50ebaa09",
       "placeholder": "",
       "style": "IPY_MODEL_b7a86172e9e945a79234b9df67c64ffc",
       "tabbable": null,
       "tooltip": null,
       "value": "495/495[1:00:58&lt;00:00,4.15s/it,Loss=0.2570,F1=0.452]"
      }
     },
     "acb678996bb94ac5bf22ef7005f2fb37": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f5e5c9a3c3f0475ab41da80c3204e5fc",
       "max": 495.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b345c24d69f649c093925027e9aa83b1",
       "tabbable": null,
       "tooltip": null,
       "value": 495.0
      }
     },
     "b0a7b09cd99c4892bf54d33544a8cf3c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b345c24d69f649c093925027e9aa83b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b6be36f9d12944af993a042374cac188": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b7a86172e9e945a79234b9df67c64ffc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "bcf0c3cd05a344c7b77d0a1e2407751c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e866cdcc8f6e4bb2b0d8f6be2da7487a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eec0023b0a324f8a8f51a3e11a9b920e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f573f18657c0461f8d1bda36cf6df116": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f5e5c9a3c3f0475ab41da80c3204e5fc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "faa46a67308b49e693a2a11c50ebaa09": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
